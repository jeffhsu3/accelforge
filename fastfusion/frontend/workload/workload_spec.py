import re
from fastfusion.frontend._set_parsing import InvertibleSet, eval_set_expression
from fastfusion.frontend.renames import Renames
from fastfusion.yamlparse.nodes import ListNode, DictNode
from ..version import assert_version
from typing import Union


CLIST_OPERATORS = [
    "EQ",
    "NE",
    "LT",
    "GT",
    "LE",
    "GE",
    "NG",
    "NL",
    "AND",
    "OR",
]

ISL_REGEX = re.compile(
    r"\b(?!(?:" + "|".join(CLIST_OPERATORS) + r")\b)[a-zA-Z#$@][a-zA-Z0-9_]*\b"
)

class CompareByName:
    def __init__(self, name: str):
        self.name = name
        
    def __str__(self):
        return self.name
    
    def __repr__(self):
        return f"{self.name}"
    
    def __eq__(self, other):
        return self.name == getattr(other, "name", other)
    
    def __hash__(self):
        return hash(self.name)
    
    def __lt__(self, other):
        return self.name < other.name
    
    def __le__(self, other):
        return self.name <= other.name
    
    def __ge__(self, other):
        return self.name >= other.name
    
    def __gt__(self, other):
        return self.name > other.name
    
    def __ne__(self, other):
        return self.name != other.name

class Tensor(CompareByName):
    pass

class RankVariable(CompareByName):
    pass
    
class Rank(CompareByName):
    pass
    
    
class Workload(DictNode):
    """
    The top-level workload object in Timeloop.

    Attributes:
        version (str): The version of the workload.
        instance (Instance): The instance object for the workload.
        shape (Shape): The shape object for the workload.
    """

    @classmethod
    def declare_attrs(cls, *args, **kwargs):
        super().declare_attrs(*args, **kwargs)
        super().add_attr("version", default="0.5", callfunc=assert_version)
        super().add_attr("einsums", EinsumList, [])
        super().add_attr("shape", ShapeDict, {})

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.version: str = self["version"]
        self.shape: Shape = self["shape"]
        self.einsums: EinsumList = self["einsums"]
        self._validate()

    def _validate(self):
        tensor2ranks = {}
        einsum_names = set()
        for einsum in self.einsums:
            if einsum.name in einsum_names:
                raise ValueError(f"Einsum name {einsum.name} is not unique")
            einsum_names.add(einsum.name)
            for tensor in einsum.tensor_accesses:
                tensor2ranks.setdefault(tensor.name, tensor.ranks)
                if tensor2ranks[tensor.name] != tensor.ranks:
                    raise ValueError(
                        f"Tensor {tensor.name} has inconsistent ranks. Found "
                        f"{tensor2ranks[tensor.name]} and {tensor.ranks}. "
                        f"Tensor is in Einsums "
                        f"{', '.join(e.name for e in self.einsums_with_tensor(tensor))}"
                    )

    def einsums_with_tensor(self, tensor: Tensor) -> list["Einsum"]:
        return [e for e in self.einsums if tensor.name in e.tensor_names]

    def tensors_read_by_einsum(self, einsum_name: str) -> set[Tensor]:
        return self.einsums[einsum_name].input_tensors()

    def tensors_written_by_einsum(self, einsum_name: str) -> set[Tensor]:
        return self.einsums[einsum_name].output_tensors()

    def einsums_that_read_tensor(self, tensor: Tensor) -> list["Einsum"]:
        return [e for e in self.einsums if tensor.name in e.input_tensors()]

    def einsums_that_write_tensor(self, tensor: Tensor) -> list["Einsum"]:
        return [e for e in self.einsums if tensor.name in e.output_tensors()]

    def get_shape_isl_string(self, einsum_name: str) -> str:
        einsum = self.einsums[einsum_name]
        einsum_shape = einsum.shape
        global_shape = [self.shape[r] for r in einsum.rank_variables if r in self.shape]
        return " and ".join(term[0] for term in einsum_shape + global_shape)

    @property
    def tensors(self) -> set[Tensor]:
        return {Tensor(t.name) for e in self.einsums for t in e.tensor_accesses}

    def mermaid_graph(self) -> str:
        """
        Get the mermaid graph of the workload.
        Returns a Mermaid graph string showing relationships between Einsums and Tensors.
        Einsums are shown as rectangles and Tensors as circles.
        """
        import mermaid as md
        from mermaid.graph import Graph
        lines = [
            "graph LR",
            "linkStyle default interpolate basis"
        ]
        
        # Add all tensors as nodes (circles)
        tensors = []
        seen_tensor_names = set()
        for einsum in self.einsums:
            lines.append(f"\tEinsum_{einsum.name}[\"<b>{einsum.name}</b>\n<small>{einsum.to_formatted_string(compress=True)}</small>\"]")
            for tensor in einsum.tensor_accesses:
                if tensor.name not in seen_tensor_names:
                    tensors.append(tensor)
                    seen_tensor_names.add(tensor.name)
                    lines.append(f"\tTensor_{tensor.name}{{{{\"<b>{tensor.name}</b>\n\"}}}}")
        
        # Add all einsums as nodes (rectangles)
        for einsum in self.einsums:
            # Add edges from tensors to einsums
            for tensor in einsum.tensor_accesses:
                if tensor.output:
                    # Output tensor: einsum -> tensor
                    lines.append(f"\tEinsum_{einsum.name} --> Tensor_{tensor.name}")
                else:
                    # Input tensor: tensor -> einsum
                    lines.append(f"\tTensor_{tensor.name} --> Einsum_{einsum.name}")
        
        # Create the graph with the flowchart script
        flowchart_script = "\n".join(lines)
        graph = Graph('Flowchart', flowchart_script)
        
        # Set the configuration to ignore node order
        config = md.Config()
        graph.config = config

        return md.Mermaid(graph)

    def get_constraint_symbol_table(
            self, 
            einsum_name: str,
            renames: Renames | None = None,
        ) -> dict[str, InvertibleSet]:
        einsum = self.einsums[einsum_name]
        inputs = einsum.input_tensors()
        outputs = einsum.output_tensors()
        all_ = inputs | outputs
        intermediates = {t for t in all_ if self.einsums_that_read_tensor(t) and self.einsums_that_write_tensor(t)}
        shared = {
            t for t in all_ if len(set(e.name for e in self.einsums_that_read_tensor(t)) | set(e.name for e in self.einsums_that_write_tensor(t))) > 1
        }

        element_to_child_space = {}
        all_rank_variables = einsum.rank_variables
        for tensor in all_:
            rank_variables = einsum.tensor_accesses[tensor.name].rank_variables
            element_to_child_space[tensor.name] = InvertibleSet(
                rank_variables,
                full_space=all_rank_variables,
                space_name=f"rank_variables",
            )
                
        kwargs = dict(
            full_space=all_,
            space_name=f"tensors",
            child_access_name="rank_variables",
            element_to_child_space=element_to_child_space,
        )
        symbol_table = {
            "All": InvertibleSet(all_, **kwargs),
            "Inputs": InvertibleSet(inputs, **kwargs),
            "Outputs": InvertibleSet(outputs, **kwargs),
            "Intermediates": InvertibleSet(intermediates, **kwargs),
            "Shared": InvertibleSet(shared, **kwargs),
            **{t.name: InvertibleSet((t,), **kwargs) for t in all_},
        }
        
        if renames is not None:
            rename = renames.get_renames_for_einsum(einsum_name)
            for rename_tensor in rename.tensor_accesses:
                einsum.output_tensors()
                name = rename_tensor.name
                source = rename_tensor.source
                injective = rename_tensor.injective
                symbol_table[name] = eval_set_expression(source, symbol_table, "tensors", injective=injective)
                
        for rank_variable in einsum.rank_variables:
            symbol_table[rank_variable] = InvertibleSet((rank_variable,), space_name="rank_variables", full_space=einsum.rank_variables)
                
        return symbol_table


class EinsumList(ListNode):
    """
    A list of einsums in the workload.
    """

    @classmethod
    def declare_attrs(cls, *args, **kwargs):
        super().declare_attrs(*args, **kwargs)
        super().add_attr("", Einsum)
        
    def __getitem__(self, key: str) -> "Einsum":
        return super().__getitem__(key)


class Einsum(DictNode):
    """
    An einsum object in the workload.
    """

    @classmethod
    def declare_attrs(cls, *args, **kwargs):
        super().declare_attrs(*args, **kwargs)
        super().add_attr("name", str)
        super().add_attr("tensor_accesses", TensorAccessList)
        super().add_attr("shape", Shape, [], callfunc=shape_factory)

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.name: str = self["name"]
        self.tensor_accesses: TensorAccessList = self["tensor_accesses"]
        self.shape: Shape = self["shape"]
        
    @property
    def rank_variables(self) -> set[RankVariable]:
        if not self.tensor_accesses:
            return set()
        return set.union(*[t.rank_variables for t in self.tensor_accesses])
    
    @property
    def tensor_names(self) -> set[str]:
        return set([t.name for t in self.tensor_accesses])
    
    @property
    def tensors(self) -> set[Tensor]:
        return set([Tensor(t.name) for t in self.tensor_accesses])
    
    @property
    def tensor2rank_variables(self) -> dict[str, set[RankVariable]]:
        return {t.name: t.rank_variables for t in self.tensor_accesses}
    
    def to_formatted_string(self, compress: bool = False) -> str:
        lhs_join = ",\n" if compress else " , "
        rhs_join = "#215;\n" if compress else " #215; "
        lhs = lhs_join.join([t.to_formatted_string() for t in self.tensor_accesses if t.output])
        rhs = rhs_join.join([t.to_formatted_string() for t in self.tensor_accesses if not t.output])
        return f"{lhs}=\n{rhs}" if compress else f"{lhs} = {rhs}"
    
    def input_tensors(self) -> set[Tensor]:
        return {Tensor(t.name) for t in self.tensor_accesses if not t.output}
    
    def output_tensors(self) -> set[Tensor]:
        return {Tensor(t.name) for t in self.tensor_accesses if t.output}

class TensorAccessList(ListNode):
    """
    A list of tensors in the workload.
    """

    @classmethod
    def declare_attrs(cls, *args, **kwargs):
        super().declare_attrs(*args, **kwargs)
        super().add_attr("", TensorAccess)

    def __getitem__(self, key: Union[str, int]) -> "TensorAccess":
        return super().__getitem__(key)

class TensorAccess(DictNode):
    """
    A tensor object.

    Attributes:
        name (str): The name of the data space.
        projection (list): The projection of the data space.
        read_write (str, bool, int): The read-write attribute of the data space.
        factors (list): The factors derived from the projection.
    """

    @classmethod
    def declare_attrs(cls, *args, **kwargs):
        super().declare_attrs(*args, **kwargs)
        super().add_attr("name", str)
        super().add_attr("projection", dict, callfunc=projection_factory)
        super().add_attr("output", (str, bool, int), False)

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.name: str = self["name"]
        self.projection: list = self["projection"]
        self.factors: list = []

        projection = [x for x in self.projection]
        while projection:
            factor = projection.pop(0)
            if isinstance(factor, list):
                projection += factor
            else:
                self.factors.append(factor)
                
    def to_formatted_string(self) -> str:
        subscript = ",".join(self.projection.values())
        if isinstance(self.projection, ImpliedProjection):
            return f"{self.name}<sub>{subscript}</sub>"
        
        string = [self.name]
        for k, v in self.projection.items():
            if len(string) < len(self.projection):
                string.append(f"<sup>{k},</sup><sub>{v},</sub>")
            else:
                string.append(f"<sup>{k}</sup><sub>{v}</sub>")
        return "".join(string)

    @property
    def ranks(self) -> tuple[Rank, ...]:
        return tuple(Rank(x) for x in self.projection.keys())

    @property
    def rank_variables(self) -> set[RankVariable]:
        # Projection values may be expressions, so we need to grab all identifiers
        return set(RankVariable(x) for x in re.findall(ISL_REGEX, " ".join(self.projection.values())))

    @property
    def relevant_rank_variables(self) -> set[RankVariable]:
        return self.rank_variables

    @property
    def fully_relevant_rank_variables(self) -> set[RankVariable]:
        return set(RankVariable(x) for x in self.projection.values() if ISL_REGEX.match(x))

    @property
    def partially_relevant_rank_variables(self) -> set[RankVariable]:
        return self.rank_variables - self.fully_relevant_rank_variables



class ImpliedProjection(dict):
    pass

def projection_factory(projection: dict | list):
    if isinstance(projection, list):
        for i, x in enumerate(projection):
            if not isinstance(x, str):
                raise TypeError(f"Element at index {i} must be a string, got {type(x)}")
            if not ISL_REGEX.match(x):
                raise ValueError(
                    f"Element '{x}' at index {i} is not a valid ISL identifier"
                    f"In a projection list, all elements must be valid ISL identifiers."
                    f"For expressions, use a dictionary projection."
                )
        projection = ImpliedProjection({x.upper(): x for x in projection})
    elif not isinstance(projection, dict):
        raise TypeError(
            f"Invalid projection: {projection}. Must be a list of "
            f"rank variables or a dictionary of rank variable to projection."
        )
    for key in projection:
        if not isinstance(key, str):
            raise TypeError(f"Invalid projection key: {key}. Must be a string.")
        if not key.isidentifier():
            raise ValueError(
                f"Invalid projection key: {key}. Must be a valid identifier."
                f"Check with the Python isidentifier() function."
            )
    return projection

class ShapeDict(DictNode):
    @classmethod
    def declare_attrs(cls, *args, **kwargs):
        super().declare_attrs(*args, **kwargs)
        super().add_attr("", Shape, part_name_match=True, no_change_key=True, callfunc=shape_factory)


def shape_factory(shape: list | str):
    if isinstance(shape, str):
        shape = [shape]
    return Shape(shape)

class Shape(ListNode):
    """
    A shape object in the workload.
    """

    @classmethod
    def declare_attrs(cls, *args, **kwargs):
        super().declare_attrs(*args, **kwargs)
        super().add_attr("", str)

    def __init__(self, *args, **kwargs):
        if len(args) == 1 and isinstance(args[0], str):
            args = [args[0]]
        super().__init__(*args, **kwargs)

    @property
    def rank_variables(self) -> set[str]:
        if not self:
            return set()
        return set.union(*[set(re.findall(ISL_REGEX, x)) for x in self])
