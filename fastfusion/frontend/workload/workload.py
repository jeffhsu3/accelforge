import re

from pydantic_core import CoreSchema
from fastfusion.util.basetypes import ParsableDict, ParsableList, ParsableModel
from fastfusion.util.setexpressions import InvertibleSet, eval_set_expression
from fastfusion.frontend.renames import Renames
from fastfusion.version import assert_version, __version__
from typing import Annotated, Any, Callable, TypeAlias
from pydantic_core.core_schema import CoreSchema, chain_schema, no_info_plain_validator_function


CLIST_OPERATORS = [
    "EQ",
    "NE",
    "LT",
    "GT",
    "LE",
    "GE",
    "NG",
    "NL",
    "AND",
    "OR",
]

ISL_REGEX = re.compile(
    r"\b(?!(?:" + "|".join(CLIST_OPERATORS) + r")\b)[a-zA-Z#$@][a-zA-Z0-9_]*\b"
)

TensorName: TypeAlias = str
RankVariableName: TypeAlias = str
RankName: TypeAlias = str

class TensorAccess(ParsableModel):
    name: TensorName
    projection: dict[str, str]
    output: bool = False
    factors: list = []

    def __init__(self, **data):
        if "projection" in data:
            data["projection"] = projection_factory(data["projection"])
        super().__init__(**data)

        projection = [x for x in self.projection.values()]
        while projection:
            factor = projection.pop(0)
            if isinstance(factor, list):
                projection += factor
            else:
                self.factors.append(factor)

    def to_formatted_string(self) -> str:
        subscript = ",".join(self.projection.values())
        if isinstance(self.projection, ImpliedProjection):
            return f"{self.name}<sub>{subscript}</sub>"

        string = [self.name]
        for k, v in self.projection.items():
            if len(string) < len(self.projection):
                string.append(f"<sup>{k},</sup><sub>{v},</sub>")
            else:
                string.append(f"<sup>{k}</sup><sub>{v}</sub>")
        return "".join(string)

    @property
    def ranks(self) -> tuple[RankName, ...]:
        return tuple(RankName(x) for x in self.projection.keys())

    @property
    def rank_variables(self) -> set[RankVariableName]:
        # Projection values may be expressions, so we need to grab all identifiers
        return set(RankVariableName(x) for x in re.findall(ISL_REGEX, " ".join(self.projection.values())))

    @property
    def relevant_rank_variables(self) -> set[RankVariableName]:
        return self.rank_variables

    @property
    def fully_relevant_rank_variables(self) -> set[RankVariableName]:
        return set(RankVariableName(x) for x in self.projection.values() if ISL_REGEX.match(x))

    @property
    def partially_relevant_rank_variables(self) -> set[RankVariableName]:
        return self.rank_variables - self.fully_relevant_rank_variables

class ImpliedProjection(dict):
    pass

def projection_factory(projection: dict | list):
    if isinstance(projection, list):
        for i, x in enumerate(projection):
            if not isinstance(x, str):
                raise TypeError(f"Element at index {i} must be a string, got {type(x)}")
            if not ISL_REGEX.match(x):
                raise ValueError(
                    f"Element '{x}' at index {i} is not a valid ISL identifier"
                    f"In a projection list, all elements must be valid ISL identifiers."
                    f"For expressions, use a dictionary projection."
                )
        projection = ImpliedProjection({x.upper(): x for x in projection})
    elif not isinstance(projection, dict):
        raise TypeError(
            f"Invalid projection: {projection}. Must be a list of "
            f"rank variables or a dictionary of rank variable to projection."
        )
    for key in projection:
        if not isinstance(key, str):
            raise TypeError(f"Invalid projection key: {key}. Must be a string.")
        if not key.isidentifier():
            raise ValueError(
                f"Invalid projection key: {key}. Must be a valid identifier."
                f"Check with the Python isidentifier() function."
            )
    return projection

def shape_factory(shape: list | str):
    if isinstance(shape, str):
        shape = [shape]
    return Shape(shape)

class Shape(ParsableList):
    @property
    def rank_variables(self) -> set[str]:
        if not self:
            return set()
        return set.union(*[set(re.findall(ISL_REGEX, x)) for x in self])

class Einsum(ParsableModel):
    name: str
    tensor_accesses: ParsableList[TensorAccess]
    shape: Shape[str] = Shape()

    @property
    def rank_variables(self) -> set[RankVariableName]:
        if not self.tensor_accesses:
            return set()
        return set.union(*[t.rank_variables for t in self.tensor_accesses])
    
    @property
    def tensor_names(self) -> set[str]:
        return set([t.name for t in self.tensor_accesses])
    
    @property
    def tensors(self) -> set[TensorName]:
        return set([TensorName(t.name) for t in self.tensor_accesses])
    
    @property
    def tensor2rank_variables(self) -> dict[str, set[RankVariableName]]:
        return {t.name: t.rank_variables for t in self.tensor_accesses}
    
    def to_formatted_string(self, compress: bool = False) -> str:
        lhs_join = ",\n" if compress else " , "
        rhs_join = "#215;\n" if compress else " #215; "
        lhs = lhs_join.join([t.to_formatted_string() for t in self.tensor_accesses if t.output])
        rhs = rhs_join.join([t.to_formatted_string() for t in self.tensor_accesses if not t.output])
        return f"{lhs}=\n{rhs}" if compress else f"{lhs} = {rhs}"
    
    def input_tensors(self) -> set[TensorName]:
        return {TensorName(t.name) for t in self.tensor_accesses if not t.output}
    
    def output_tensors(self) -> set[TensorName]:
        return {TensorName(t.name) for t in self.tensor_accesses if t.output}

class Workload(ParsableModel):
    version: Annotated[str, assert_version] = __version__
    einsums: ParsableList[Einsum] = []
    shape: ParsableDict[str, str] = {}

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._validate()

    def _validate(self):
        tensor2ranks = {}
        einsum_names = set()
        for einsum in self.einsums:
            if einsum.name in einsum_names:
                raise ValueError(f"Einsum name {einsum.name} is not unique")
            einsum_names.add(einsum.name)
            for tensor_accesses in einsum.tensor_accesses:
                tensor2ranks.setdefault(tensor_accesses.name, tensor_accesses.ranks)
                if tensor2ranks[tensor_accesses.name] != tensor_accesses.ranks:
                    raise ValueError(
                        f"TensorName {tensor_accesses.name} has inconsistent ranks. Found "
                        f"{tensor2ranks[tensor_accesses.name]} and {tensor_accesses.ranks}. "
                        f"TensorName is in Einsums "
                        f"{', '.join(e.name for e in self.einsums_with_tensor(tensor_accesses))}"
                    )

    def einsums_with_tensor(self, tensor: TensorName) -> list["Einsum"]:
        return [e for e in self.einsums if tensor in e.tensor_names]

    def tensors_read_by_einsum(self, einsum_name: str) -> set[TensorName]:
        return self.einsums[einsum_name].input_tensors()

    def tensors_written_by_einsum(self, einsum_name: str) -> set[TensorName]:
        return self.einsums[einsum_name].output_tensors()

    def einsums_that_read_tensor(self, tensor: TensorName) -> list["Einsum"]:
        return [e for e in self.einsums if tensor in e.input_tensors()]

    def einsums_that_write_tensor(self, tensor: TensorName) -> list["Einsum"]:
        return [e for e in self.einsums if tensor in e.output_tensors()]

    def get_shape_isl_string(self, einsum_name: str) -> str:
        einsum = self.einsums[einsum_name]
        einsum_shape = einsum.shape
        global_shape = [self.shape[r] for r in einsum.rank_variables if r in self.shape]
        return " and ".join(term for term in einsum_shape + global_shape)

    @property
    def tensors(self) -> set[TensorName]:
        return {TensorName(t.name) for e in self.einsums for t in e.tensor_accesses}

    def mermaid_graph(self) -> str:
        """
        Get the mermaid graph of the workload.
        Returns a Mermaid graph string showing relationships between Einsums and Tensors.
        Einsums are shown as rectangles and Tensors as circles.
        """
        import mermaid as md
        from mermaid.graph import Graph
        lines = [
            "graph LR",
            "linkStyle default interpolate basis"
        ]
        
        # Add all tensors as nodes (circles)
        tensors = []
        seen_tensor_names = set()
        for einsum in self.einsums:
            lines.append(f"\tEinsum_{einsum.name}[\"<b>{einsum.name}</b>\n<small>{einsum.to_formatted_string(compress=True)}</small>\"]")
            for tensor_access in einsum.tensor_accesses:
                if tensor_access.name not in seen_tensor_names:
                    tensors.append(tensor)
                    seen_tensor_names.add(tensor_access.name)
                    lines.append(f"\tTensor_{tensor_access.name}{{{{\"<b>{tensor_access.name}</b>\n\"}}}}")
        
        # Add all einsums as nodes (rectangles)
        for einsum in self.einsums:
            # Add edges from tensors to einsums
            for tensor_access in einsum.tensor_accesses:
                if tensor_access.output:
                    # Output tensor: einsum -> tensor
                    lines.append(f"\tEinsum_{einsum.name} --> Tensor_{tensor_access.name}")
                else:
                    # Input tensor: tensor -> einsum
                    lines.append(f"\tTensor_{tensor_access.name} --> Einsum_{einsum.name}")
        
        # Create the graph with the flowchart script
        flowchart_script = "\n".join(lines)
        graph = Graph('Flowchart', flowchart_script)
        
        # Set the configuration to ignore node order
        config = md.Config()
        graph.config = config

        return md.Mermaid(graph)

    def get_constraint_symbol_table(
            self, 
            einsum_name: str,
            renames: Renames | None = None,
        ) -> dict[str, InvertibleSet]:
        einsum = self.einsums[einsum_name]
        inputs = einsum.input_tensors()
        outputs = einsum.output_tensors()
        all_ = inputs | outputs
        intermediates = {t for t in all_ if self.einsums_that_read_tensor(t) and self.einsums_that_write_tensor(t)}
        shared = {
            t for t in all_ if len(set(e.name for e in self.einsums_that_read_tensor(t)) | set(e.name for e in self.einsums_that_write_tensor(t))) > 1
        }

        element_to_child_space = {}
        all_rank_variables = einsum.rank_variables
        for tensor in all_:
            rank_variables = einsum.tensor_accesses[tensor].rank_variables
            element_to_child_space[tensor] = InvertibleSet(
                instance=rank_variables,
                full_space=all_rank_variables,
                space_name=f"rank_variables",
            )
                
        kwargs = dict(
            full_space=all_,
            space_name=f"tensors",
            child_access_name="rank_variables",
            element_to_child_space=element_to_child_space,
        )
        symbol_table = {
            "All": InvertibleSet(instance=all_, **kwargs),
            "Inputs": InvertibleSet(instance=inputs, **kwargs),
            "Outputs": InvertibleSet(instance=outputs, **kwargs),
            "Intermediates": InvertibleSet(instance=intermediates, **kwargs),
            "Shared": InvertibleSet(instance=shared, **kwargs),
            **{t: InvertibleSet(instance=(t,), **kwargs) for t in all_},
        }
        
        if renames is not None:
            rename = renames.get_renames_for_einsum(einsum_name)
            for rename_tensor in rename.tensor_accesses:
                einsum.output_tensors()
                name = rename_tensor.name
                source = rename_tensor.source
                injective = rename_tensor.injective
                symbol_table[name] = eval_set_expression(source, symbol_table, "tensors", injective=injective)
                
        for rank_variable in einsum.rank_variables:
            symbol_table[rank_variable] = InvertibleSet(instance=(rank_variable,), space_name="rank_variables", full_space=einsum.rank_variables)
                
        return symbol_table