import copy
from collections import defaultdict
from numbers import Number
from typing import Callable, Iterator, List

from pandas import DataFrame
from tqdm import tqdm

import fastfusion.frontend.architecture as architecture
from fastfusion.frontend.mapping import (
    Compute,
    Iteration,
    Mapping,
    MappingNode,
    Storage,
    Temporal,
)
from fastfusion.frontend.specification import Specification
from fastfusion.frontend.workload.isl import get_rank_variable_bounds
from fastfusion.frontend.workload.workload import (
    Einsum,
    EinsumName,
    RankVariableName,
    Workload,
)

from fastfusion.mapper.FFM.compress_pmappings import (
    COMPRESSED_INDEX_COLUMN,
    compress_df,
)
from fastfusion.mapper.FFM.exploration.mapper_one_einsum.dataflow_generator import (
    get_storage_choices,
)
from fastfusion.mapper.FFM.exploration.tile_shape_exploration import (
    explore_tile_shapes,
    get_initial_delta_choices,
)
from fastfusion.mapper.FFM.joining.sim import SIM
from fastfusion.mapper.FFM.pareto import (
    MAPPING_COLUMN,
    PartialMappings,
    col2nameloop,
    col_used_in_pareto,
    is_reservation_col,
    nameloop2col,
    tensor2col,
)
from fastfusion.mapper.FFM.exploration.contraints.constraints import (
    MappingConstraints,
    get_constraints,
)
from fastfusion.mapper.FFM.tags import Tags
from fastfusion.frontend.mapping import Reservation as ReservationNode
from fastfusion.mapper.FFM.exploration.mapper_one_einsum.loop_generator import (
    insert_temporal_loops,
    insert_spatial_loops,
)
from fastfusion.mapper.FFM.exploration.mapper_one_einsum.mapper_job import (
    Job,
    SameCompatibilityJobs
)


def unpack_loops_to_rank_variables(mapping: List[MappingNode]):
    mapping_new = []
    for node in mapping:
        if not isinstance(node, Iteration) or not isinstance(node.rank_variable, set):
            mapping_new.append(node)
            continue

        for r in sorted(node.rank_variable):
            mapping_new.append(
                type(node)(
                    rank_variable=r,
                    **node.model_dump(exclude={"rank_variable"}),
                )
            )
    return mapping_new


def label_fused_loops(mapping: List[MappingNode]):
    last_backing_storage = None
    for i, node in enumerate(mapping):
        if isinstance(node, Storage) and node._backing:
            last_backing_storage = i
    if last_backing_storage is None:
        raise ValueError(
            f"No backing storage found in mapping {", ".join(m.compact_string() for m in mapping)}"
        )

    for i, node in enumerate(mapping):
        if isinstance(node, Iteration):
            node._fused = i < last_backing_storage
    return mapping


# =================================================================================================
# Iterate over mappings
# =================================================================================================
def temporal_fused_constraint_thing_fix_me(
    mapping: List[MappingNode],
    rank_variables: list[RankVariableName],
    rank_variable_bounds: dict[RankVariableName, int],
):
    # Only one fused loop is allowed per rank variable
    rank_variables = list(rank_variables)
    if not rank_variables:
        yield mapping
        return

    my_rank_variable = RankVariableName(rank_variables.pop())
    # indent = " " * (10 - len(rank_variables))
    fused_loops = [
        i
        for i, node in enumerate(mapping)
        if isinstance(node, Iteration)
        and node._fused
        and my_rank_variable == node.rank_variable
        and rank_variable_bounds[my_rank_variable]
        > 1  # Don't worry about loops with size 1
    ]

    if not fused_loops or len(fused_loops) == 1:
        # print(indent + f"Yielding for rank variable {my_rank_variable}. Length: {len(mapping)}")
        # print(indent + ", ".join(m.compact_string() for m in mapping))
        yield from temporal_fused_constraint_thing_fix_me(
            mapping, rank_variables, rank_variable_bounds
        )
        return

    for choice in fused_loops:
        mapping_new = list(mapping)
        for f in fused_loops[::-1]:
            if f != choice:
                mapping_new.pop(f)
        # print(indent + f"Yielding for rank variable {my_rank_variable}. Length: {len(mapping_new)}")
        # print(indent + ", ".join(m.compact_string() for m in mapping_new))
        yield from temporal_fused_constraint_thing_fix_me(
            mapping_new, rank_variables, rank_variable_bounds
        )


def temporal_constraint_2_fix_me(mapping: List[MappingNode], einsum: Einsum):
    return mapping
    # Between two storage nodes for the same memory, pop all loops that index
    # into the tensors for both storage nodes.
    # return mapping
    to_pop = set()
    for i, node in enumerate(mapping):
        for j, node2 in enumerate(mapping):
            if i >= j:
                continue
            if not isinstance(node, Storage) or not isinstance(node2, Storage):
                continue
            if node.memory != node2.memory:
                continue
            rv1 = set.union(*(einsum.tensor2rank_variables[t] for t in node.tensors))
            rv2 = set.union(*(einsum.tensor2rank_variables[t] for t in node2.tensors))
            to_drop = rv1 & rv2
            if not to_drop:
                continue
            for k in range(i + 1, j):
                if (
                    isinstance(mapping[k], Temporal)
                    and mapping[k].rank_variable in to_drop
                ):
                    to_pop.add(k)
    return [node for i, node in enumerate(mapping) if i not in to_pop]


def place_missing_temporal_loops(mapping: List[MappingNode], einsum: Einsum):
    # If any rank variables are missing, add them as high as possible.
    rank_variables = einsum.rank_variables
    for m in mapping:
        if isinstance(m, Temporal) and not m._fused:
            rank_variables.discard(m.rank_variable)

    # insert_point = 0
    # while insert_point < len(mapping) and not isinstance(mapping[insert_point], Temporal):
    #     insert_point += 1
    # Insert point: Right under the last backing storage
    for i in range(len(mapping) - 1, -1, -1):
        if isinstance(mapping[i], Storage) and mapping[i]._backing:
            insert_point = i + 1
            break

    temporals = [
        Temporal(rank_variable=r, tile_shape="symbol") for r in sorted(rank_variables)
    ]

    if insert_point == len(mapping):
        mapping.extend(temporals)
    else:
        for t in temporals:
            mapping.insert(insert_point, t)


def pad_with_bottom_loops(mapping: list[MappingNode], einsum: Einsum):
    rank_variables = einsum.rank_variables
    rank_var_to_count = defaultdict(lambda: 0)
    for node in mapping:
        if isinstance(node, Temporal):
            rank_var_to_count[node.rank_variable] += 1

    for rank_var in rank_variables:
        if rank_var_to_count[rank_var] < 2:
            mapping.append(Temporal(rank_variable=rank_var, tile_shape="symbol"))


def timeloop_style_even(mapping: list[MappingNode]):
    # Iterate through the mapping. If there are >2 storage nodes for the same
    # memory, combine all but the innermost one
    memory2indices = defaultdict(list)
    i = 0
    for i, node in enumerate(mapping):
        if not isinstance(mapping[i], Storage):
            i += 1
            continue
        seen = memory2indices[node.memory]
        if len(seen) <= 1:
            seen.append(i)
        else:
            mapping[i] = None
            mapping[seen[-1]].tensors.extend(node.tensors)
    return [m for m in mapping if m is not None]


def get_ranks_with_tile_pattern(producer_name: EinsumName, workload: Workload):
    initial_choices = get_initial_delta_choices(producer_name, workload)
    return {
        rank_var
        for rank_var in workload.einsums[producer_name].rank_variables
        if len(initial_choices[rank_var]) > 1
    }


def iterate_mappings_no_constraints(
    spec: Specification,
    einsum_name: str,
    arch_flattened: list[architecture.Leaf],
    rank_variable_bounds: dict[RankVariableName, int],
    except_from_imperfect: set,
):
    first_memory = None
    for node in arch_flattened:
        if isinstance(node, architecture.Memory):
            first_memory = node
            break
    if first_memory is None:
        raise ValueError("No memory found in architecture")

    ranks_with_tile_pattern = get_ranks_with_tile_pattern(einsum_name, spec.workload)
    # print('RANKS WITH TILE PATTERN', ranks_with_tile_pattern)

    symbol_table = spec.workload.get_constraint_symbol_table(einsum_name, spec.renames)
    einsum = spec.workload.einsums[einsum_name]
    for mapping, symbol_table in get_storage_choices(
        arch_flattened, symbol_table, spec
    ):
        mapping = copy.deepcopy(mapping)
        if spec.mapper_ffm.timeloop_style_even:
            mapping = timeloop_style_even(mapping)
        # print(", ".join(m.compact_string() for m in mapping))
        for mapping in insert_temporal_loops(
            mapping,
            einsum,
            first_memory,
            rank_variable_bounds,
            ranks_with_tile_pattern,
            spec.workload,
            except_from_imperfect,
        ):
            mapping = copy.deepcopy(mapping)
            # print(", ".join(m.compact_string() for m in mapping))
            insert_spatial_loops(mapping, einsum, arch_flattened)
            # print(", ".join(m.compact_string() for m in mapping))
            mapping = unpack_loops_to_rank_variables(mapping)
            label_fused_loops(mapping)
            # print('POST-LABEL')
            # print(", ".join(m.compact_string() for m in mapping))
            # print(f'{einsum_name}: {", ".join(m.compact_string() for m in mapping)}')
            for mapping2 in temporal_fused_constraint_thing_fix_me(
                mapping,
                list(spec.workload.einsums[einsum_name].rank_variables),
                rank_variable_bounds,
            ):  # TODO
                # mapping2 = temporal_constraint_2_fix_me(mapping2, einsum)
                # print('PRE-PADDING')
                # print(", ".join(m.compact_string() for m in mapping2))
                place_missing_temporal_loops(mapping, einsum)
                # pad_with_bottom_loops(mapping2, einsum)
                # print('POST-PADDING')
                # print(", ".join(m.compact_string() for m in mapping2))
                # print('FINAL')
                # print(", ".join(m.compact_string() for m in mapping))

                yield mapping2, symbol_table


def iterate_mappings_constraints(
    spec: Specification,
    einsum_names: list[str] | str | None = None,
    arch_flattened: list[architecture.Leaf] | None = None,
    rank_variable_bounds: dict[RankVariableName, int] | None = None,
    except_from_imperfect: set = set(),
) -> Iterator[tuple[Mapping, MappingConstraints]]:
    if arch_flattened is None:
        arch_flattened = spec.get_flattened_architecture()
    compute_name = arch_flattened[-1].name

    if isinstance(einsum_names, str):
        einsum_names = [einsum_names]
    if einsum_names is None:
        einsum_names = [e.name for e in spec.workload.einsums]

    if rank_variable_bounds is None:
        rank_variable_bounds = get_rank_variable_bounds(spec, einsum_names)

    for einsum_name in einsum_names:
        for mapping, symbol_table in iterate_mappings_no_constraints(
            spec,
            einsum_name,
            arch_flattened,
            rank_variable_bounds,
            except_from_imperfect,
        ):
            # MAPPING MUST NOT BE MODIFIED AFTER THIS POINT
            mapping, constraints = get_constraints(
                arch_flattened, mapping, symbol_table, einsum_name
            )
            mapping.append(Compute(einsum=einsum_name, compute=compute_name))
            mapping = Mapping(nodes=[copy.copy(n) for n in mapping])
            yield mapping, constraints


# =================================================================================================
# Make sims
# =================================================================================================


def get_equivalent_sims(
    sim: SIM, tagger: Callable[[Mapping], Tags], reservation_levels: set[int]
) -> list[SIM]:
    equivalent_permutations = sim.compatibility.make_equivalent_permutations(
        reservation_levels
    )
    result = []
    for c in equivalent_permutations:
        try:
            tags = Tags() if tagger is None else tagger(c)
            result.append(SIM(c.update(tags=tags), None))
        except ValueError:
            pass
    return result


def get_compatibility_loops(mapping: Mapping, tile_shapes: list[int]) -> "Mapping":
    compatibility = Mapping(nodes=[])
    i = 0
    for node in mapping.nodes:
        while i < len(tile_shapes) and tile_shapes[i] is None:
            i += 1
        if i >= len(tile_shapes):
            break
        new_node = copy.deepcopy(node)
        if isinstance(node, Iteration):
            new_node.tile_shape = tile_shapes[i]
            i += 1
        compatibility.nodes.append(new_node)
    return compatibility


def shift_reservations_by_null_loop_indices(
    mappings: DataFrame, null_loop_indices: set[int]
):
    prev = copy.deepcopy(mappings)  # TODO: Is this needed?
    target2newabovename = {}
    dropcols = []
    for c in mappings.columns:
        if not is_reservation_col(c):
            continue
        name, above = col2nameloop(c)
        new_above = above - sum(above > i for i in null_loop_indices)
        target = nameloop2col(name, new_above)
        if target in target2newabovename:
            if above > target2newabovename[target][1]:
                dropcols.append(nameloop2col(*target2newabovename[target]))
                target2newabovename[target] = (name, above)
            else:
                dropcols.append(c)
        else:
            target2newabovename[target] = (name, above)

    mappings.drop(columns=dropcols, inplace=True)
    renames = {}
    for target, (name, above) in target2newabovename.items():
        renames[nameloop2col(name, above)] = target
    mappings.rename(columns=renames, inplace=True)
    if len(mappings.columns) != len(mappings.columns.unique()):
        shift_reservations_by_null_loop_indices(prev, null_loop_indices)
        raise ValueError(f"Duplicate columns: {mappings.columns}")
    assert len(mappings.columns) == len(mappings.columns.unique())
    return mappings


# =================================================================================================
# Top level
# =================================================================================================
def get_single_einsum_jobs(job: Job) -> list[Job]:
    workload = job.spec.workload
    mappings_constraints = tqdm(
        iterate_mappings_constraints(
            job.spec,
            job.einsum_name,
            job.flattened_arch,
            job.rank_variable_bounds,
            job.except_from_imperfect,
        ),
        desc=f"Generating storage and loop choices for Einsum {job.einsum_name}",
    )

    jobs = []
    for i, (mapping, constraints) in enumerate(mappings_constraints):
        new_job = copy.deepcopy(job)
        new_job.mapping = mapping
        new_job.constraints = constraints
        new_job.job_id = job.job_id + i
        new_job.flattened_arch = job.flattened_arch
        new_job.rank_variable_bounds = get_rank_variable_bounds(
            job.spec.workload, job.einsum_name
        )
        jobs.append(new_job)

    return jobs


def generate_pmappings(
    jobs_with_similar_compatibilities: SameCompatibilityJobs
):
    total_pmappings = 0
    results = []

    job_ids = [job.job_id for job in jobs_with_similar_compatibilities]
    for job in jobs_with_similar_compatibilities:
        result, n_pmappings = explore_tile_shapes(
            job.mapping,
            job.constraints,
            job.spec,
            job.flattened_arch,
            job.metrics,
            _fix_me=False,
        )
        # This changes the pmapping count to include permutations
        # n_loops = []
        # cur_n_loops = 0
        # for node in job.mapping.nodes:
        #     if isinstance(node, Iteration):
        #         cur_n_loops += 1
        #     elif isinstance(node, ReservationNode):
        #         if cur_n_loops >= 1:
        #             n_loops.append(cur_n_loops)
        #         cur_n_loops = 0
        # if cur_n_loops >= 1:
        #     n_loops.append(cur_n_loops)
        # Uncomment below to include permutations
        # n_pmappings *= math.prod(math.factorial(len(job.rank_variable_bounds)) for n in n_loops)

        # Uncomment below to not include permutations AND assume that the permutation
        # engine has no knowledge of relevant/irrelevant rank variables. Note that the
        # space will still be much bigger than reported since the extra loops would also
        # increase the index factorization space size.

        # n_pmappings *= math.prod(math.factorial(n) for n in n_loops)
        # result[MAPPING_COLUMN] = [job.mapping] * len(result)
        result[COMPRESSED_INDEX_COLUMN] = job.job_id
        total_pmappings += n_pmappings
        results.append(result)

    compatibility = jobs_with_similar_compatibilities.compatibility
    intermediate_tensors = jobs_with_similar_compatibilities.intermediate_tensors
    einsum_name = jobs_with_similar_compatibilities.einsum_name
    rank_variable_bounds = jobs_with_similar_compatibilities.rank_variable_bounds
    tagger = jobs_with_similar_compatibilities.tagger

    # Creating a PartialMappings fills in reservation columns since different partial
    # mappings have different ones.
    next_shared_loop_index = len(compatibility.loops) - 1
    results = PartialMappings.concat(
        [
            PartialMappings(
                r,
                skip_pareto=True,
                next_shared_loop_index=next_shared_loop_index,
            )
            for r in results
        ]
    ).data

    if results.empty:
        return einsum_name, [], None, job_ids

    n_tile_shapes = sum(
        1 if isinstance(l.bound, Number) else 2 for l in compatibility.loops
    )
    fused_loop_cols = [f"__tile_shape{i}" for i in range(n_tile_shapes)]
    tensor2size_cols = [tensor2col(t) for t in intermediate_tensors]
    pareto_cols = [c for c in results.columns if col_used_in_pareto(c)]
    compress_cols = [
        c for c in results.columns if c not in tensor2size_cols and c not in pareto_cols
    ]

    jobs_passed_pareto = sorted(results[COMPRESSED_INDEX_COLUMN].unique())

    extra_data = {
        job.job_id: {f"{job.einsum_name}{MAPPING_COLUMN}": job.mapping}
        for job in jobs_with_similar_compatibilities
        if job.job_id in jobs_passed_pareto
    }

    results, decompress_data = compress_df(
        df=results,
        einsum_name=einsum_name,
        keep_columns=pareto_cols + tensor2size_cols + fused_loop_cols,
        compress_columns=compress_cols,
        extra_data=extra_data,
    )

    if fused_loop_cols:
        groups = list(results.groupby(fused_loop_cols))
    else:
        groups = [((), results)]

    pmappings_per_group = (
        None if total_pmappings is None else total_pmappings / len(groups)
    )

    sims = []

    seen_compatibilities = set()

    for tile_shape, mappings in groups:
        tensor2size = {}

        dropcols = list(fused_loop_cols)
        for tensor in intermediate_tensors:  # Sizes are all the same
            tensor2size[tensor] = mappings[tensor2col(tensor)].iloc[0]
            dropcols.append(tensor2col(tensor))
        mappings.drop(columns=dropcols, inplace=True)

        new_compatibility, null_loop_indices = compatibility.populate_tile_shape(
            tile_shape, rank_variable_bounds, tensor2size
        )

        shift_reservations_by_null_loop_indices(mappings, null_loop_indices)

        # TODO: Redundant capacity checks because limit_capacity is called. We want it
        # so we can drop dead reservations though.fcompress_dfz
        # Skip pareto because we already did it above
        # prev_len = len(mappings)
        next_shared_loop_index_this_group = len(new_compatibility.loops) - 1
        partial_mappings = PartialMappings(
            mappings,
            next_shared_loop_index=next_shared_loop_index_this_group,
            n_pmappings=pmappings_per_group,
            skip_pareto=next_shared_loop_index_this_group == next_shared_loop_index,
        )
        reservation_levels = partial_mappings.all_reservation_levels()
        sim = SIM(new_compatibility, partial_mappings)

        sim._equivalent_sims = get_equivalent_sims(sim, tagger, reservation_levels)
        sim._equivalent_sims = [
            e
            for e in sim._equivalent_sims
            if e.compatibility not in seen_compatibilities
        ]
        seen_compatibilities.update(e.compatibility for e in sim._equivalent_sims)
        sims.append(sim)

    return einsum_name, sims, decompress_data, jobs_passed_pareto