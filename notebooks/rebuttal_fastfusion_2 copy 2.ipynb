{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING Loading configuration file from /root/.config/fastfusion/config.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTE ENERGY / 8 ????????????\n",
      "COMPUTE ENERGY / 8 ????????????\n",
      "COMPUTE ENERGY / 8 ????????????\n",
      "COMPUTE ENERGY / 8 ????????????\n",
      "COMPUTE ENERGY / 8 ????????????\n",
      "Overall area budget: 8448.022488990722 mm^2\n",
      "Global buffer area: 0.02256514974783053. Area remaining: 0.00825458610917197\n",
      "Global buffer area: 0.011728085273632725. Area remaining: 0.00825458610917197\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Global buffer: 128 MB, Local buffer: 1 MB, MAC dims: 128x128\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tensor order and loop choices for Einsum QK_softmax: 2it [00:00, 29.25it/s]\n",
      "Generating tensor order and loop choices for Einsum I: 1it [00:00, 16.10it/s]\n",
      "Generating tensor order and loop choices for Einsum QK: 8it [00:00, 54.71it/s]]\n",
      "Generating tensor order and loop choices for Einsum FFA: 8it [00:00, 67.06it/s]\n",
      "Generating tensor order and loop choices for Einsum Q: 8it [00:00, 57.86it/s]]]\n",
      "Generating tensor order and loop choices for Einsum FFB: 8it [00:00, 63.86it/s]\n",
      "Generating tensor order and loop choices for Einsum V: 8it [00:00, 53.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 job for I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tensor order and loop choices for Einsum Z: 8it [00:00, 65.87it/s]\n",
      "Generating tensor order and loop choices for Einsum K: 8it [00:00, 63.95it/s]\n",
      "Generating tensor order and loop choices for Einsum AV: 8it [00:00, 49.13it/s]\n",
      "Generating jobs: 100%|██████████| 10/10 [00:01<00:00,  5.62it/s]\n",
      "WARNING Insufficient jobs available to utilize available threads. Splitting jobs into smaller chunks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8 jobs for V\n",
      "Generated 8 jobs for K\n",
      "Generated 8 jobs for Q\n",
      "Generated 8 jobs for QK\n",
      "Generated 2 jobs for QK_softmax\n",
      "Generated 8 jobs for AV\n",
      "Generated 8 jobs for Z\n",
      "Generated 8 jobs for FFA\n",
      "Generated 8 jobs for FFB\n",
      "AV\n",
      "FFB\n",
      "I_in\n",
      "WK\n",
      "K\n",
      "QK_softmax\n",
      "WQ\n",
      "Z\n",
      "WFFA\n",
      "FFA\n",
      "WV\n",
      "WFFB\n",
      "I\n",
      "Q\n",
      "V\n",
      "QK\n",
      "WZ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pmappings: 100%|██████████| 67/67 [00:11<00:00,  5.79it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sims' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 185\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m100\u001b[39m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m parameterization \u001b[38;5;129;01min\u001b[39;00m parameterizations: \u001b[38;5;66;03m# \"fuse\"\u001b[39;00m\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# while True:\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m#     try:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     x, mappings = \u001b[43mget_fused_mappings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmac_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmac_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mllb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mglb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmac_energy\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43madder\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiplier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameterization\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameterization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_latency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_latency\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m         \u001b[38;5;66;03m# break\u001b[39;00m\n\u001b[32m    196\u001b[39m         \u001b[38;5;66;03m# except Exception as e:\u001b[39;00m\n\u001b[32m    197\u001b[39m         \u001b[38;5;66;03m#     max_latency *= 2\u001b[39;00m\n\u001b[32m    198\u001b[39m         \u001b[38;5;66;03m#     print(f\"Error: {e}\")\u001b[39;00m\n\u001b[32m    199\u001b[39m     \u001b[38;5;66;03m# max_latency = x\u001b[39;00m\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x != \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 132\u001b[39m, in \u001b[36mget_fused_mappings\u001b[39m\u001b[34m(spec, pe_x, pe_y, local_buffer_model, global_buffer_model, tagger, parameterization, return_mappings, mac_energy, max_latency)\u001b[39m\n\u001b[32m    130\u001b[39m pmappings = make_pmappings(spec)\n\u001b[32m    131\u001b[39m pmapping_time = time.time() - t0\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m total_pmappings = \u001b[38;5;28msum\u001b[39m(p.mappings.n_pmappings \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[43msims\u001b[49m.values() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m v)\n\u001b[32m    133\u001b[39m n_pareto_optimal_mappings = \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(p.mappings.data) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m sims.values() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m v)\n\u001b[32m    134\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTook \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpmapping_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds to generate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_pmappings\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m partial mappings (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_pmappings\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39mpmapping_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m per second). \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_pareto_optimal_mappings\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pareto optimal mappings (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_pareto_optimal_mappings\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39mtotal_pmappings*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m% of total).\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'sims' is not defined"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "from hwcomponents_cacti import SRAM as CactiSRAM\n",
    "from hwcomponents_library import AladdinAdder, AladdinMultiplier\n",
    "\n",
    "from fastfusion.frontend.architecture import Memory\n",
    "from fastfusion.frontend.specification import Specification\n",
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_sims\n",
    "from fastfusion.mapper.simanneal.wrappers import join_sims\n",
    "\n",
    "import copy\n",
    "import time\n",
    "from fastfusion import Specification\n",
    "from fastfusion.mapper.metrics import Metrics\n",
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_sims\n",
    "from fastfusion.mapper.FFM.joining.sim import SIM\n",
    "from fastfusion.mapper.FFM.joining.simexplore import join_sims\n",
    "import fastfusion.mapper.FFM.exploration.mapper_one_einsum as mapper_one_einsum\n",
    "\n",
    "from fastfusion.mapper.FFM.exploration.mapping_filter_tags.ffmt import get_ffmt_tag\n",
    "from fastfusion.mapper.FFM.exploration.mapping_filter_tags.onesplit import get_one_split_tag\n",
    "from fastfusion.mapper.FFM.pareto import PartialMappings\n",
    "from fastfusion.mapper.FFM import make_pmappings, join_pmappings\n",
    "\n",
    "# TODO: area is an alias for get_area\n",
    "# TODO: Separate energy and area\n",
    "# TODO: Move scaling into main hwcomponents repo\n",
    "# TODO: Function that just returns the hwcomponents component\n",
    "# TODO: Is all the initial right consolidation necessary?\n",
    "# TODO: Datawidth calculation for energy\n",
    "\n",
    "# TODO: Reference specific tensor names in constraints, even if those tensors are not in\n",
    "# a particular Einsum. Also have the error mrssages for parsing errors list which Einsum\n",
    "# failed. Einsums that aren't in the tensor should resolve to NotInThisEinsum(), which =\n",
    "# nothing.\n",
    "\n",
    "# TODO: Make a setting for the below two in the spec\n",
    "# TODO: Generate pmappings one Einsum at a time. Once we've made compatibility, check it\n",
    "# against the previously-generated compatibilities and stop if there's no match.\n",
    "# TODO: Once the previous is done, also add a forward check. Once the compatibilities of\n",
    "# a particular Einsum are generated, we can immediately check the previous Einsums.\n",
    "# TODO: Make the mapping return an object that supports union operators and stuff\n",
    "# TODO: The fix in mapping.py\n",
    "\n",
    "# TODO: have inf a supported value in YAMLs\n",
    "# TODO: programatically check if any storages are below all backing storages. If so,\n",
    "# don't record reservations for it.\n",
    "# TODO: If any memroies have size > sum of all tensor sizes, also don't record reservations\n",
    "\n",
    "spec = Specification.from_yaml(\n",
    "    f\"architecture/four_level.arch.yaml\",\n",
    "    \"workloads/mha_full.workload.yaml\",\n",
    "    \"workloads/mha_full.renames.yaml\",\n",
    ")\n",
    "\n",
    "adder = AladdinAdder(tech_node=7e-9, width=16)\n",
    "multiplier = AladdinMultiplier(tech_node=7e-9, width=8)\n",
    "mac_area = adder.area + multiplier.area\n",
    "\n",
    "base_local_buffer_size = 4 * 1024 * 1024 * 8\n",
    "base_local_buffer = CactiSRAM(tech_node=7e-9, width=128, depth=base_local_buffer_size // 128)\n",
    "base_global_buffer_size = 128 * 1024 * 1024 * 8\n",
    "base_global_buffer = CactiSRAM(tech_node=7e-9, width=1024, depth=base_global_buffer_size // 1024)\n",
    "area_budget = (mac_area * 128 * 128 + base_local_buffer.area) * 4 + base_global_buffer.area\n",
    "\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "\n",
    "objective = lambda df: df['metric_Latency']# * df['metric_Energy']\n",
    "\n",
    "def get_fused_mappings(\n",
    "        spec: Specification, \n",
    "        pe_x,\n",
    "        pe_y,\n",
    "        local_buffer_model,\n",
    "        global_buffer_model,\n",
    "        tagger=None, \n",
    "        # fuse=True,\n",
    "        parameterization=\"\",\n",
    "        return_mappings=False,\n",
    "        mac_energy: float = None,\n",
    "        max_latency: float = None\n",
    "    ) -> PartialMappings:\n",
    "    cachekey = (pe_x, pe_y, local_buffer_model.width, local_buffer_model.depth, global_buffer_model.width, global_buffer_model.depth, parameterization)\n",
    "    fname = parameterization + \" \" + hashlib.md5(str(cachekey).encode()).hexdigest()\n",
    "    if os.path.exists(f\"cache/{fname}.pkl\"):\n",
    "        print(f\"Loading from cache: {fname}\")\n",
    "        mappings = pickle.load(open(f\"cache/{fname}.pkl\", \"rb\"))\n",
    "        if return_mappings:\n",
    "            return mappings\n",
    "        return objective(mappings.data).min(), mappings\n",
    "    spec = copy.deepcopy(spec)\n",
    "    local_buffer: Memory = spec.architecture.nodes[\"LocalBuffer\"]\n",
    "    local_buffer.attributes.size = local_buffer_model.width * local_buffer_model.depth\n",
    "    global_buffer: Memory = spec.architecture.nodes[\"GlobalBuffer\"]\n",
    "    global_buffer.attributes.size = global_buffer_model.width * global_buffer_model.depth\n",
    "    if mac_energy is not None:\n",
    "        mac = spec.architecture.nodes[\"MAC\"]\n",
    "        mac.actions[\"compute\"].arguments.energy = mac_energy / 8\n",
    "    for target, model in [(local_buffer, local_buffer_model), (global_buffer, global_buffer_model)]:\n",
    "        target.actions[\"read\"].arguments.energy = model.read() / model.width\n",
    "        target.actions[\"write\"].arguments.energy = model.write() / model.width\n",
    "    main_memory: Memory = spec.architecture.nodes[\"MainMemory\"]\n",
    "    if parameterization == \"Unfused\":\n",
    "        main_memory.constraints.tensors.keep = \"All()\"\n",
    "    elif parameterization == \"FlashAttention B\":\n",
    "        main_memory.constraints.tensors.keep = \"All() - (I | Q | K | V | QK | QK_softmax)\"# - QK_softmax\"# - Q - K - V - I\"\n",
    "        main_memory.constraints.tensors.bypass = \"I | Q | K | V | QK | QK_softmax\"#Q | K | V | I\"# | QK | FFA\"\n",
    "    elif parameterization == \"FlashAttention A\":\n",
    "        main_memory.constraints.tensors.keep = \"All() - (QK | QK_softmax)\"# - QK_softmax\"# - Q - K - V - I\"\n",
    "        main_memory.constraints.tensors.bypass = \"QK | QK_softmax\"#Q | K | V | I\"# | QK | FFA\"\n",
    "    elif parameterization == \"FFM\":\n",
    "        main_memory.constraints.tensors.keep = \"~Intermediates()\" #\"# | AV | Z \"\n",
    "        main_memory.constraints.tensors.bypass = \"I | Q | K | V | QK\"#Q | K | V | I\"# | QK | FFA\"\n",
    "        pass\n",
    "    else:\n",
    "        assert False, f\"Parameterization {parameterization} not supported\"\n",
    "    register: Memory = spec.architecture.nodes[\"Register\"]\n",
    "    register.spatial.fanout[\"X\"] = pe_x\n",
    "    register.spatial.fanout[\"Y\"] = pe_y\n",
    "    \n",
    "    spec.calculate_component_energy_area()\n",
    "    # flattened_architecture = spec.get_flattened_architecture()\n",
    "    # t0 = time.time()\n",
    "    # sims, pmapping_objects = get_sims(spec, flattened_architecture, tagger=tagger, metrics=Metrics.LATENCY | Metrics.ENERGY)\n",
    "    pmappings = make_pmappings(spec)\n",
    "    # pmapping_time = time.time() - t0\n",
    "    # total_pmappings = sum(p.mappings.n_pmappings for v in sims.values() for p in v)\n",
    "    # n_pareto_optimal_mappings = sum(len(p.mappings.data) for v in sims.values() for p in v)\n",
    "    # print(f'Took {pmapping_time:.2f} seconds to generate {total_pmappings} partial mappings ({total_pmappings / pmapping_time:.2f} per second). {n_pareto_optimal_mappings} pareto optimal mappings ({n_pareto_optimal_mappings / total_pmappings*100:.2f}% of total).')\n",
    "    # t0 = time.time()\n",
    "    mappings = join_pmappings(spec, pmappings)\n",
    "    # join_time = time.time() - t0\n",
    "    # print(f\"Pmappings: {pmapping_time:.2f}. Joining: {join_time:.2f}. Total Pmappings: {total_pmappings}. Total mappings: {mappings.n_pmappings}. Time per pmapping: {pmapping_time / total_pmappings:.2e}\")\n",
    "    pickle.dump(mappings, open(f\"cache/{fname}.pkl\", \"wb\"))\n",
    "    if return_mappings:\n",
    "        return mappings\n",
    "    return objective(mappings.data).min(), mappings\n",
    "\n",
    "print(f'Overall area budget: {area_budget * 1e6} mm^2')\n",
    "\n",
    "parameterization2edp = {}\n",
    "parameterization2mappings = {}\n",
    "\n",
    "parameterizations = [\"Unfused\", \"FlashAttention A\", \"FlashAttention B\", \"FFM\"]#, \"Unfused\"] # \"FFM\", \"Unfused\", \"FlashAttention\"]#, \"FlashAttention\", \"Unfused\"]\n",
    "\n",
    "TARGET_TECH_NODE = 4e-9\n",
    "adder = AladdinAdder(tech_node=TARGET_TECH_NODE, width=16)\n",
    "multiplier = AladdinMultiplier(tech_node=TARGET_TECH_NODE, width=8)\n",
    "mac_area = adder.area + multiplier.area\n",
    "\n",
    "glb_size = 512 * 1024 * 1024 * 8\n",
    "glb = CactiSRAM(tech_node=TARGET_TECH_NODE, width=1024, depth=glb_size // 1024)\n",
    "llb_size = 1 * 1024 * 1024 * 8\n",
    "llb = CactiSRAM(tech_node=TARGET_TECH_NODE, width=128, depth=llb_size // 128)\n",
    "\n",
    "# for glb_MB in [8, 16, 32, 64, 128, 256, 512, 1024]:#, 64, 128]:#, 64, 256]:#,16]:#16, 32, 64, 128]: # [16, 32, 64, 128]: # 16, 64\n",
    "# for glb_MB in [1024, 512, 256, 128, 64, 32, 16, 8]:\n",
    "for mac_x, mac_y in [(128,128), (256,128), (256,256), (512, 256), (512,512), (1024, 512)]:\n",
    "    total_mac_area = mac_area * mac_x * mac_y\n",
    "    area_remaining = area_budget - 4 * (llb.area + total_mac_area)\n",
    "    while glb.area > area_remaining:\n",
    "        print(f\"Global buffer area: {glb.area}. Area remaining: {area_remaining}\")\n",
    "        glb_size //= 2\n",
    "        glb = CactiSRAM(tech_node=TARGET_TECH_NODE, width=1024, depth=glb_size // 1024)\n",
    "        if area_remaining < 0:\n",
    "            break\n",
    "    max_latency = None\n",
    "    \n",
    "    glb_MB = glb_size // 1024 // 1024 // 8\n",
    "    llb_MB = llb_size // 1024 // 1024 // 8\n",
    "    \n",
    "    print(f\"\\n\\n\")\n",
    "    print(f\"=\" * 100)\n",
    "    print(f\"Global buffer: {glb_MB} MB, Local buffer: {llb_MB} MB, MAC dims: {mac_x}x{mac_y}\")\n",
    "    print(f\"=\" * 100)\n",
    "\n",
    "    for parameterization in parameterizations: # \"fuse\"\n",
    "        # while True:\n",
    "        #     try:\n",
    "        x, mappings = get_fused_mappings(\n",
    "            spec,\n",
    "            mac_x,\n",
    "            mac_y,\n",
    "            llb,\n",
    "            glb,\n",
    "            mac_energy=(adder.add() + multiplier.multiply()),\n",
    "            parameterization=parameterization,\n",
    "            max_latency=max_latency\n",
    "        )\n",
    "            # break\n",
    "            # except Exception as e:\n",
    "            #     max_latency *= 2\n",
    "            #     print(f\"Error: {e}\")\n",
    "        # max_latency = x\n",
    "        if x != 0:\n",
    "            parameterization2edp[f\"{parameterization} {glb_MB}MB {llb_MB}MB {mac_x}x{mac_y}\"] = x\n",
    "            parameterization2mappings[f\"{parameterization} {glb_MB}MB {llb_MB}MB {mac_x}x{mac_y}\"] = mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "# plt.rcParams.update({'font.size': 28})\n",
    "\n",
    "def plot_default_formatting(ax, grid_axis='both'):\n",
    "    ax.tick_params(axis='both', which='major')#, labelsize=20)\n",
    "    ax.tick_params(axis='both', which='minor')#, labelsize=20)\n",
    "    legend = ax.legend()\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    legend.get_frame().set_edgecolor('black')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('black')\n",
    "    if ax.get_legend() is None:\n",
    "        legend = ax.legend(fontsize=24, ncol=2)\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(axis=grid_axis, which='major', linestyle='-', linewidth='0.3', color='gray')\n",
    "    ax.grid(axis=grid_axis, which='minor', linestyle='--', linewidth='0.1', color='lightgray')\n",
    "    \n",
    "\n",
    "def make_bar_chart(\n",
    "    data,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    "    y_scale,\n",
    "    output_file=None,\n",
    "    normalize: bool = False,\n",
    "    ylim=(None, None),\n",
    "    xlim=(None, None),\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a bar chart from the given data and save it as a PDF.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    if isinstance(data, dict) and isinstance(next(iter(data.values())), dict):\n",
    "        bar_width = 0.8 / len(data)\n",
    "        keys = list(next(iter(data.values())).keys())\n",
    "        x = range(len(keys))\n",
    "        first = next(iter(data.values()))\n",
    "            \n",
    "        for i, (label, values) in enumerate(data.items()):\n",
    "            bar_positions = [pos + i * bar_width for pos in x]\n",
    "            to_plot = values\n",
    "            if normalize:\n",
    "                to_plot = {k: v / first[k] for k, v in values.items()}\n",
    "            bars = plt.bar(bar_positions, to_plot.values(), width=bar_width, label=label)\n",
    "        plt.xticks([pos + (len(data) - 1) * bar_width / 2 for pos in x], keys)\n",
    "        plt.legend(loc='upper right', fontsize=10)\n",
    "    else:\n",
    "        keys = list(data.keys())\n",
    "        bars = plt.bar(keys, data.values())\n",
    "\n",
    "    # Set logarithmic scale for Y-axis if specified\n",
    "    if y_scale == 'log':\n",
    "        plt.yscale('log')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlim(xlim)\n",
    "\n",
    "    # Rotate X-axis labels vertically\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plot_default_formatting(plt.gca(), grid_axis='y')\n",
    "    \n",
    "    if output_file is not None:\n",
    "        with open(output_file, 'wb') as f:\n",
    "            plt.savefig(f, format='pdf', bbox_inches='tight')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "print()\n",
    "make_bar_chart(\n",
    "    parameterization2edp,\n",
    "    title=None,\n",
    "    xlabel=None,\n",
    "    ylabel=\"EDP\",\n",
    "    y_scale='linear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # glb_MB = 128\n",
    "# # sram_MB = 4\n",
    "# # parameterization = \"\"\n",
    "\n",
    "# # cur_area_budget = area_budget\n",
    "# # glb_size = glb_MB * 1024 * 1024 * 8\n",
    "# # glb = CactiSRAM(tech_node=7e-9, width=1024, depth=glb_size // 1024)\n",
    "# # cur_area_budget -= glb.area\n",
    "# # # for sram_MB in [0.25, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4]:\n",
    "# # sram_size = sram_MB * 1024 * 1024 * 8\n",
    "# # llb = CactiSRAM(tech_node=7e-9, width=128, depth=sram_size // 128)\n",
    "# # remaining_area = cur_area_budget / 4 - llb.area # Per-MXU\n",
    "# # mac_dims = int((remaining_area / mac_area) ** 0.5)\n",
    "# # print(f\"Global buffer: {glb_MB} MB, Local buffer: {sram_MB} MB, MAC dims: {mac_dims}x{mac_dims}\")\n",
    "# # print(f'GLB read energy: {glb.read()}. LLB read energy: {llb.read()}')\n",
    "\n",
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_per_tensor_size, get_num_computes\n",
    "for tensor, size in sorted(get_per_tensor_size(spec).items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{tensor}: {size}\")\n",
    "print(f\"Number of computes: {get_num_computes(spec)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_rank_variable_bounds_for_all_einsums\n",
    "\n",
    "parameterization2latencycols: list[dict[str, float]] = []\n",
    "for p, mappings in parameterization2mappings.items():\n",
    "    mappings._data = mappings.data.sort_values(by=\"metric_Latency\", ascending=True)\n",
    "    rank_variable_bounds = get_rank_variable_bounds_for_all_einsums(spec)\n",
    "\n",
    "    row = {\n",
    "        \"Parameterization\": p,\n",
    "    }\n",
    "    for col in mappings.data.columns:\n",
    "        print(f'{col}: {mappings.data.iloc[0][col]}')\n",
    "        # if \"Latency\" in col:\n",
    "        # if \"metric_Latency\" in col:\n",
    "        if \"Latency\" in col:\n",
    "        # if \"metric_Energy\" in col:\n",
    "            row[col] = mappings.data.iloc[0][col]\n",
    "    parameterization2latencycols.append(row)\n",
    "\n",
    "    # from fastfusion.mapper.FFM.visualization import make_mapping\n",
    "    # from IPython.display import SVG\n",
    "    # newmapping = make_mapping(mappings.data.iloc[0], spec.workload.einsum_names, get_rank_variable_bounds_for_all_einsums(spec))\n",
    "    # for col in mappings.data.columns:\n",
    "    #     print(f'{col}: {mappings.data.iloc[0][col]}')\n",
    "\n",
    "    # display(SVG(newmapping.render()))\n",
    "    \n",
    "from fastfusion.accelerated_imports.pd import DataFrame\n",
    "df = DataFrame(parameterization2latencycols)\n",
    "from fastfusion.accelerated_imports import pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "df\n",
    "    \n",
    "# {'n1'}-1 || [GlobalBuffer] T1 sz 0 above 1\n",
    "# TODO: Re-add -1 to the mapper one eisnum freenig\n",
    "# compatibility2sims['Matmul1'][\"{'n1'}-1 || [GlobalBuffer] T1 sz 0 above 1\"]\n",
    "# Above 1: 8192\n",
    "# Above 2: 8321\n",
    "# compatibility2sims['Matmul2'][\"{'n1'}-1 || [GlobalBuffer] T1 sz 0 above 1, [GlobalBuffer] T2 sz 0 above 0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_rank_variable_bounds_for_all_einsums\n",
    "\n",
    "mac_dims = int((((area_budget - glb.area) / 4 - llb.area) / mac_area)** 0.5)\n",
    "mappings = list(parameterization2mappings.values())[0]\n",
    "mappings._data = mappings.data.sort_values(by=\"metric_Latency\", ascending=True).head()\n",
    "rank_variable_bounds = get_rank_variable_bounds_for_all_einsums(spec)\n",
    "from fastfusion.mapper.FFM.visualization import make_mapping\n",
    "from IPython.display import SVG\n",
    "newmapping = make_mapping(mappings.data.iloc[0], spec.workload.einsum_names, get_rank_variable_bounds_for_all_einsums(spec))\n",
    "a = {}\n",
    "for col in mappings.data.columns:\n",
    "    print(f'{col}: {mappings.data.iloc[0][col]}')\n",
    "    if \"Latency\" in col:\n",
    "        a[col] = mappings.data.iloc[0][col]\n",
    "display(SVG(newmapping.render()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False\n",
    "\n",
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_rank_variable_bounds_for_all_einsums\n",
    "\n",
    "sram_size = 0.5 * 1024 * 1024 * 8\n",
    "llb = CactiSRAM(tech_node=7e-9, width=128, depth=sram_size // 128)\n",
    "mac_dims = int((((area_budget - glb.area) / 4 - llb.area) / mac_area)** 0.5)\n",
    "mappings = get_fused_mappings(\n",
    "    spec,\n",
    "    mac_dims,\n",
    "    llb,\n",
    "    glb,\n",
    "    return_mappings=True,\n",
    "    parameterization=\"FFM\"\n",
    ")\n",
    "mappings._data = mappings.data.sort_values(by=\"metric_Latency\", ascending=True).head()\n",
    "rank_variable_bounds = get_rank_variable_bounds_for_all_einsums(spec)\n",
    "from fastfusion.mapper.FFM.visualization import make_mapping\n",
    "from IPython.display import SVG\n",
    "newmapping = make_mapping(mappings.data.iloc[0], spec.workload.einsum_names, get_rank_variable_bounds_for_all_einsums(spec))\n",
    "b = {}\n",
    "for col in mappings.data.columns:\n",
    "    print(f'{col}: {mappings.data.iloc[0][col]}')\n",
    "    if \"Latency\" in col:\n",
    "        b[col] = mappings.data.iloc[0][col]\n",
    "display(SVG(newmapping.render()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([a, b])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
