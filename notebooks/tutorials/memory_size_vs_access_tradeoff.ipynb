{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe9944d",
   "metadata": {},
   "source": [
    "## Tradeoffs in Memory Size vs. Accesses to Parent Memories\n",
    "\n",
    "This notebook will show you how to analyze tradeoffs in memory size versus accesses to\n",
    "parent memories. The generated curve will show you, for a given memory size, the lower\n",
    "bound of the number of accesses to parent memories.\n",
    "\n",
    "This analysis is called Orojenesis, and it is introduced in \"Mind the Gap: Attainable\n",
    "Data Movement and Operational Intensity Bounds for Tensor Algorithms\" by Qijing Huang,\n",
    "Po-An Tsai, Joel S. Emer, Angshuman Parashar. The paper includes additional analysis and\n",
    "example use cases for this tradeoff.\n",
    "\n",
    "Our plan to do this analysis is the following:\n",
    "- Set up a simple architecture with a main memory and a global buffer\n",
    "- Tell the mapper to optimize for both main memory accesses and global buffer usage\n",
    "- Observe the resulting Pareto frontier between global buffer usage (size) and the\n",
    "  minimum number of accesses to main memory.\n",
    "\n",
    "To this end, we have an \"orojenesis\" architecture in the examples/arches directory.\n",
    "Let's take a look at it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a0ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import accelforge as af\n",
    "\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "``` yaml\n",
    "{open(af.examples.arches.orojenesis).read()}\n",
    "```\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63668ed9",
   "metadata": {},
   "source": [
    "First import AccelForge and initialize a Spec. We'll use a 4096x4096x4096 matrix\n",
    "multiply as our workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9af75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelforge as af\n",
    "from pathlib import Path\n",
    "\n",
    "examples_dir = Path(\"../../examples\")\n",
    "\n",
    "spec = af.Spec.from_yaml(\n",
    "    af.examples.arches.orojenesis,\n",
    "    af.examples.workloads.matmuls,\n",
    "    jinja_parse_data={\"N_EINSUMS\": 1, \"M\": 4096, \"KN\": 4096},\n",
    ")\n",
    "spec.mapper.metrics = af.Metrics.ENERGY | af.Metrics.RESOURCE_USAGE\n",
    "results = spec.map_workload_to_arch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c06b6c5",
   "metadata": {},
   "source": [
    "Let's plot the results. We can see that, on a log-log scale, the curve appears to be\n",
    "linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9b08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results.data.sort_values(\"Total<SEP>energy\", inplace=True)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.plot(\n",
    "    [x * spec.arch.find(\"GlobalBuffer\").size for x in results.resource_usage()[\"GlobalBuffer\"]],\n",
    "    results.energy(),\n",
    "    marker=\"o\",\n",
    "    drawstyle=\"steps-pre\"\n",
    ")\n",
    "ax.set_xlabel(\"On-Chip Memory Size (bits)\")\n",
    "ax.set_ylabel(\"Lowest-Attainable Off-Chip Accesses (bits)\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee35e9",
   "metadata": {},
   "source": [
    "### Fused vs. Unfused\n",
    "\n",
    "Let's also do a comparison of how the fusion affects this curve. We'll use the same\n",
    "architecture, but this time with a larger workload. We'll run a GPT-3 6.7B attention\n",
    "head with 8192 tokens.\n",
    "\n",
    "Note: Depending on your machine, this may take 3-15 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelforge as af\n",
    "from pathlib import Path\n",
    "\n",
    "examples_dir = Path(\"../../examples\")\n",
    "\n",
    "spec = af.Spec.from_yaml(\n",
    "    af.examples.arches.orojenesis,\n",
    "    af.examples.workloads.gpt3_6_7B,\n",
    "    # af.examples.workloads.three_matmuls_annotated,\n",
    "    jinja_parse_data={\"N_TOKENS\": 8192}\n",
    ")\n",
    "spec.mapper.metrics = af.Metrics.ENERGY | af.Metrics.RESOURCE_USAGE\n",
    "\n",
    "# FUSED\n",
    "spec.arch.find(\"MainMemory\").tensors.keep = \"~Intermediates\"\n",
    "spec.arch.find(\"MainMemory\").tensors.may_keep = \"All\"\n",
    "results_fused = spec.map_workload_to_arch()\n",
    "\n",
    "# UNFUSED\n",
    "spec.arch.find(\"MainMemory\").tensors.keep = \"All\"\n",
    "results_unfused = spec.map_workload_to_arch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb84dfd",
   "metadata": {},
   "source": [
    "Let's plot the results. \n",
    "\n",
    "\n",
    "We can see that, for small on-chip memory sizes, there is an approximately-linear\n",
    "relationship between on-chip memory size and the number of off-chip accesses.\n",
    "Additionally in this region, fusion has negligible benefit. These are because the\n",
    "on-chip memory is too small to fully reuse the tensors of any given Einsum, so off-chip\n",
    "accesses are dominated by refetching tensors.\n",
    "\n",
    "As we increase on-chip memory size, the curve begins to flatten out, and fusion begins\n",
    "to have a larger effect. These are because the available on-chip memory is large enough\n",
    "to store full tensors, which enables multiple fusion opportunities:\n",
    "\n",
    "- Untiled fusion: We can generate a full output tensor, keep it on-chip, then use it for\n",
    "  the next Einsum\n",
    "- Tiled fusion: We can keep a full weight tensor on-chip, while consuming input tensor\n",
    "  and sending output tensors one-tile-at-a-time from and to other Einsums without going\n",
    "  off-chip.\n",
    "\n",
    "Also note that fusion tends to be less helpful if we're refetching tensors multiple\n",
    "times from off-chip, because fusion can save, at most, one fetch of a tensor per Einsum\n",
    "(exchanging with another Einsum happens once). If we're refetching tensors multiple\n",
    "times, we're likely better off using all available capacity to minimize refetches.\n",
    "\n",
    "The curve slowly flattens out, rather than quickly, because each Einsum has a different\n",
    "shape, and some Einsums enter the no-refetch, fusion-helpful region at smaller on-chip\n",
    "memory sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aba6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_unfused.resource_usage()\n",
    "results_unfused.columns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "results_fused.data.sort_values(\"Total<SEP>energy\", inplace=True)\n",
    "ax.plot(\n",
    "    np.array(results_fused.resource_usage()[\"GlobalBuffer\"]) * spec.arch.find(\"GlobalBuffer\").size,\n",
    "    results_fused.energy(),\n",
    "    label=\"Fused\",\n",
    "    marker=\"o\",\n",
    "    drawstyle=\"steps-pre\"\n",
    ")\n",
    "\n",
    "results_unfused.data.sort_values(\"Total<SEP>energy\", inplace=True)\n",
    "ax.plot(\n",
    "    np.array(results_unfused.resource_usage()[\"GlobalBuffer\"]) * spec.arch.find(\"GlobalBuffer\").size,\n",
    "    results_unfused.energy(),\n",
    "    label=\"Unfused\",\n",
    "    marker=\"o\",\n",
    "    drawstyle=\"steps-pre\"\n",
    ")\n",
    "ax.set_xlabel(\"On-Chip Memory Size (bits)\")\n",
    "ax.set_ylabel(\"Lowest-Attainable Off-Chip Accesses (bits)\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
