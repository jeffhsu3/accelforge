{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CactiSRAM' from 'hwcomponents_cacti' (/usr/local/lib/python3.13/site-packages/hwcomponents_cacti/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhwcomponents_cacti\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CactiSRAM\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhwcomponents_library\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AladdinAdder, AladdinMultiplier\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfastfusion\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfrontend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marchitecture\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Memory\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'CactiSRAM' from 'hwcomponents_cacti' (/usr/local/lib/python3.13/site-packages/hwcomponents_cacti/__init__.py)"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "from hwcomponents_cacti import SRAM\n",
    "from hwcomponents_library import AladdinAdder, AladdinMultiplier\n",
    "\n",
    "from fastfusion.frontend.architecture import Memory\n",
    "from fastfusion.frontend.specification import Specification\n",
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_sims\n",
    "from fastfusion.mapper.simanneal.wrappers import join_sims\n",
    "\n",
    "import copy\n",
    "import time\n",
    "from fastfusion import Specification\n",
    "from fastfusion.mapper.FFM.exploration.metrics import Metrics\n",
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_sims\n",
    "from fastfusion.mapper.FFM.joining.sim import SIM\n",
    "from fastfusion.mapper.FFM.joining.simexplore import join_sims\n",
    "import fastfusion.mapper.FFM.exploration.mapper_one_einsum as mapper_one_einsum\n",
    "\n",
    "from fastfusion.mapper.FFM.exploration.mapping_filter_tags.ffmt import get_ffmt_tag\n",
    "from fastfusion.mapper.FFM.exploration.mapping_filter_tags.onesplit import get_one_split_tag\n",
    "from fastfusion.mapper.FFM.pareto import PartialMappings\n",
    "\n",
    "spec = Specification.from_yaml(\n",
    "    f\"architecture/four_level.arch.yaml\",\n",
    "    \"workloads/mha_full.workload.yaml\",\n",
    "    \"workloads/mha_full.renames.yaml\",\n",
    ")\n",
    "\n",
    "# adder = AladdinAdder(technology=\"7nm\", width=16)\n",
    "# multiplier = AladdinMultiplier(technology=\"7nm\", width=8)\n",
    "# mac_area = adder.get_area() + multiplier.get_area()\n",
    "\n",
    "# base_local_buffer_size = 4 * 1024 * 1024 * 8\n",
    "# base_local_buffer = CactiSRAM(technology=\"7nm\", width=128, depth=base_local_buffer_size // 128)\n",
    "# base_global_buffer_size = 128 * 1024 * 1024 * 8\n",
    "# base_global_buffer = CactiSRAM(technology=\"7nm\", width=1024, depth=base_global_buffer_size // 1024)\n",
    "# area_budget = (mac_area * 128 * 128 + base_local_buffer.get_area()) * 4 + base_global_buffer.get_area()\n",
    "\n",
    "# TARGET_TECH_NODE = \"4nm\"\n",
    "# adder = AladdinAdder(technology=TARGET_TECH_NODE, width=16)\n",
    "# multiplier = AladdinMultiplier(technology=TARGET_TECH_NODE, width=8)\n",
    "# mac_area = adder.get_area() + multiplier.get_area()\n",
    "# base_local_buffer_size = 1 * 1024 * 1024 * 8\n",
    "# base_local_buffer = CactiSRAM(technology=TARGET_TECH_NODE, width=128, depth=base_local_buffer_size // 128)\n",
    "# base_global_buffer_size = 16.5 * 1024 * 1024 * 8\n",
    "# base_global_buffer = CactiSRAM(technology=TARGET_TECH_NODE, width=1024, depth=base_global_buffer_size // 1024)\n",
    "# area_budget = (mac_area * 512 * 1024 + base_local_buffer.get_area()) * 4 + base_global_buffer.get_area()\n",
    "\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "\n",
    "objective = lambda df: df['metric_Latency']# * df['metric_Energy']\n",
    "\n",
    "def get_fused_mappings(\n",
    "        spec: Specification,\n",
    "        n_pes,\n",
    "        pe_x,\n",
    "        pe_y,\n",
    "        tagger=None, \n",
    "        # fuse=True,\n",
    "        parameterization=\"\",\n",
    "        return_mappings=False,\n",
    "        max_latency: float = None\n",
    "    ) -> PartialMappings:\n",
    "    print(f'Running parameterization {parameterization}')\n",
    "    cachekey = (n_pes, pe_x, pe_y, parameterization)\n",
    "    fname = parameterization + \" \" + hashlib.md5(str(cachekey).encode()).hexdigest()\n",
    "    if os.path.exists(f\"cache/{fname}.pkl\") and parameterization != \"FlashAttention B\" and parameterization != \"FFM\":\n",
    "        print(f\"Loading from cache: {fname}\")\n",
    "        mappings = pickle.load(open(f\"cache/{fname}.pkl\", \"rb\"))\n",
    "        if return_mappings:\n",
    "            return mappings\n",
    "        return objective(mappings.data).min(), mappings\n",
    "    spec = copy.deepcopy(spec)\n",
    "    # local_buffer: Memory = spec.architecture.nodes[\"LocalBuffer\"]\n",
    "    # local_buffer.attributes.size = local_buffer_model.width * local_buffer_model.depth\n",
    "    # local_buffer.spatial.fanout[\"X\"] = n_pes\n",
    "    global_buffer: Memory = spec.architecture.nodes[\"GlobalBuffer\"]\n",
    "    main_memory: Memory = spec.architecture.nodes[\"MainMemory\"]\n",
    "    if parameterization == \"Unfused\":\n",
    "        main_memory.constraints.storage.keep = \"All()\"\n",
    "    elif parameterization == \"FlashAttention B\":\n",
    "        main_memory.constraints.storage.keep = \"All() - (Q | K | QK | QK_softmax)\"# - QK_softmax\"# - Q - K - V - I\"\n",
    "        main_memory.constraints.storage.bypass = \"Q | K | QK | QK_softmax\"#Q | K | V | I\"# | QK | FFA\"\n",
    "    elif parameterization == \"FlashAttention A\":\n",
    "        main_memory.constraints.storage.keep = \"All() - (QK | QK_softmax)\"# - QK_softmax\"# - Q - K - V - I\"\n",
    "        main_memory.constraints.storage.bypass = \"QK | QK_softmax\"#Q | K | V | I\"# | QK | FFA\"\n",
    "    elif parameterization == \"FFM\":\n",
    "        main_memory.constraints.storage.keep = \"~Intermediates()\" #\"# | AV | Z \"\n",
    "        # main_memory.constraints.storage.bypass = \"I | Q | K | V | QK\"#Q | K | V | I\"# | QK | FFA\"\n",
    "        pass\n",
    "    elif parameterization == \"Weight-Stationary\":\n",
    "        main_memory.constraints.storage.keep = \"~Intermediates()\" #\"# | AV | Z \"\n",
    "        global_buffer.constraints.dataflow.storage_orders.append([\"weight\", \"input\", \"output\"])\n",
    "    elif parameterization == \"Input-Stationary\":\n",
    "        global_buffer.constraints.dataflow.storage_orders.append([\"input\", \"output\", \"weight\"])\n",
    "    elif parameterization == \"Output-Stationary\":\n",
    "        global_buffer.constraints.dataflow.storage_orders.append([\"output\", \"weight\", \"input\"])\n",
    "    elif parameterization == \"Fixed-Dataflow\":\n",
    "        main_memory.constraints.storage.keep = \"~Intermediates() | weight\"\n",
    "        global_buffer.constraints.dataflow.storage_orders.append([\n",
    "            \"MainMemory.tensors() & weight\", \n",
    "            \"MainMemory.tensors() & input\", \n",
    "            \"MainMemory.tensors() & output\", \n",
    "            \"~MainMemory.tensors() & weight\", \n",
    "            \"~MainMemory.tensors() & input\", \n",
    "            \"~MainMemory.tensors() & output\",\n",
    "        ])\n",
    "    else:\n",
    "        assert False, f\"Parameterization {parameterization} not supported\"\n",
    "        \n",
    "    register: Memory = spec.architecture.nodes[\"Register\"]\n",
    "    register.spatial.fanout[\"X\"] = pe_x\n",
    "    register.spatial.fanout[\"Y\"] = pe_y\n",
    "    register.spatial.fanout[\"Z\"] = n_pes\n",
    "    \n",
    "    spec.estimate_energy_area()\n",
    "    flattened_architecture = spec.get_flattened_architecture()\n",
    "    t0 = time.time()\n",
    "    sims, decompress_data = get_sims(spec, flattened_architecture, tagger=tagger, metrics=Metrics.LATENCY | Metrics.ENERGY) # metrics=Metrics.ENERGY | # | Metrics.PER_COMPONENT_ENERGY)\n",
    "    pmapping_time = time.time() - t0\n",
    "    total_pmappings = sum(p.mappings.n_pmappings for v in sims.values() for p in v)\n",
    "    n_pareto_optimal_mappings = sum(len(p.mappings.data) for v in sims.values() for p in v)\n",
    "    print(f'Took {pmapping_time:.2f} seconds to generate {total_pmappings} partial mappings ({total_pmappings / pmapping_time:.2f} per second). {n_pareto_optimal_mappings} pareto optimal mappings ({n_pareto_optimal_mappings / total_pmappings*100:.2f}% of total).')\n",
    "    t0 = time.time()\n",
    "    mappings = join_sims(sims, spec, flattened_architecture)\n",
    "    join_time = time.time() - t0\n",
    "    mappings.decompress(decompress_data)\n",
    "    print(f\"Pmappings: {pmapping_time:.2f}. Joining: {join_time:.2f}. Total Pmappings: {total_pmappings}. Total mappings: {mappings.n_pmappings}. Time per pmapping: {pmapping_time / total_pmappings:.2e}\")\n",
    "    pickle.dump(mappings, open(f\"cache/{fname}.pkl\", \"wb\"))\n",
    "    if return_mappings:\n",
    "        return mappings\n",
    "    return objective(mappings.data).min(), mappings\n",
    "\n",
    "# print(f'Overall area budget: {area_budget * 1e6} mm^2')\n",
    "\n",
    "parameterization2edp = {}\n",
    "parameterization2mappings = {}\n",
    "\n",
    "parameterizations = [\n",
    "    \"Unfused\",\n",
    "    \"Fixed-Dataflow\",\n",
    "    \"FlashAttention A\",\n",
    "    \"FlashAttention B\",\n",
    "    \"FFM\"\n",
    "]#, \"Unfused\"] # \"FFM\", \"Unfused\", \"FlashAttention\"]#, \"FlashAttention\", \"Unfused\"]\n",
    "\n",
    "# TARGET_TECH_NODE = \"4nm\"\n",
    "# adder = AladdinAdder(technology=TARGET_TECH_NODE, width=16)\n",
    "# multiplier = AladdinMultiplier(technology=TARGET_TECH_NODE, width=8)\n",
    "# mac_area = adder.get_area() + multiplier.get_area()\n",
    "\n",
    "# # glb_size = 512 * 1024 * 1024 * 8\n",
    "# # glb = CactiSRAM(technology=TARGET_TECH_NODE, width=1024, depth=glb_size // 1024)\n",
    "# llb_size = 1 * 1024 * 1024 * 8\n",
    "# llb = CactiSRAM(technology=TARGET_TECH_NODE, width=128, depth=llb_size // 128)\n",
    "\n",
    "def binary_search_glb_size(area_remaining):\n",
    "    print(f\"Area remaining: {area_remaining}\")\n",
    "    # Binary search to nearest 1MB\n",
    "    low = 1\n",
    "    high = 1024\n",
    "    while low < high:\n",
    "        mid = round((low + high) / 2)\n",
    "        size = mid * 1024 * 1024 * 8\n",
    "        glb = CactiSRAM(technology=TARGET_TECH_NODE, width=1024, depth=size // 1024)\n",
    "        if glb.get_area() > area_remaining:\n",
    "            if mid == high:\n",
    "                break\n",
    "            print(f\"High {high} -> {mid}\")\n",
    "            high = mid\n",
    "        else:\n",
    "            if mid == low:\n",
    "                break\n",
    "            print(f\"Low {low} -> {mid}\")\n",
    "            low = mid\n",
    "    return low * 1024 * 1024 * 8\n",
    "\n",
    "mac_x, mac_y = 128, 128\n",
    "# for mac_x, mac_y in [(256, 256), (1024, 1024), (1024, 512), (512, 512), (512, 256)]:\n",
    "# for n_pes in [256, 1, 2, 4, 8, 16, 32, 64, 128, 256]:#2, 4, 8, 16, 32]:\n",
    "for n_pes in [256, 64]:#\n",
    "    # total_mac_area = mac_area * mac_x * mac_y\n",
    "    # area_remaining = area_budget - n_pes * (llb.get_area() + total_mac_area)\n",
    "    # if area_remaining < 0:\n",
    "    #     continue\n",
    "    # glb_size = binary_search_glb_size(area_remaining)\n",
    "    # glb = CactiSRAM(technology=TARGET_TECH_NODE, width=1024, depth=glb_size // 1024)\n",
    "    # glb_size = 128 * 1024 * 1024 * 8\n",
    "    # glb = CactiSRAM(technology=TARGET_TECH_NODE, width=1024, depth=glb_size // 1024)\n",
    "    max_latency = None\n",
    "    \n",
    "    # glb_MB = glb_size // 1024 // 1024 // 8\n",
    "    # llb_MB = llb_size // 1024 // 1024 // 8\n",
    "    \n",
    "    print(f\"\\n\\n\")\n",
    "    print(f\"=\" * 100)\n",
    "    print(f\"MAC dims: {mac_x}x{mac_y}\")\n",
    "    print(f\"=\" * 100)\n",
    "\n",
    "    for parameterization in parameterizations: # \"fuse\"\n",
    "        while True:\n",
    "            try:\n",
    "                x, mappings = get_fused_mappings(\n",
    "                    spec,\n",
    "                    n_pes,\n",
    "                    mac_x,\n",
    "                    mac_y,\n",
    "                    parameterization=parameterization,\n",
    "                    max_latency=max_latency if parameterization != \"FlashAttention B\" else None\n",
    "                )\n",
    "                break\n",
    "            except Exception as e:\n",
    "                max_latency *= 2\n",
    "                print(f\"Error: {e}\")\n",
    "        if \"Fixed-Dataflow\" not in parameterization:\n",
    "            max_latency = x\n",
    "        if x != 0:\n",
    "            parameterization2edp[f\"{parameterization} {n_pes} {mac_x}x{mac_y}\"] = x\n",
    "            parameterization2mappings[f\"{parameterization} {n_pes} {mac_x}x{mac_y}\"] = mappings\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "spec = Specification.from_yaml(\n",
    "    f\"architecture/four_level_no_weight_par.arch.yaml\",\n",
    "    \"workloads/mha_full_long_sequence.workload.yaml\",\n",
    "    \"workloads/mha_full.renames.yaml\",\n",
    ")\n",
    "\n",
    "# adder = AladdinAdder(technology=\"7nm\", width=16)\n",
    "# multiplier = AladdinMultiplier(technology=\"7nm\", width=8)\n",
    "# mac_area = adder.get_area() + multiplier.get_area()\n",
    "\n",
    "# base_local_buffer_size = 4 * 1024 * 1024 * 8\n",
    "# base_local_buffer = CactiSRAM(technology=\"7nm\", width=128, depth=base_local_buffer_size // 128)\n",
    "# base_global_buffer_size = 128 * 1024 * 1024 * 8\n",
    "# base_global_buffer = CactiSRAM(technology=\"7nm\", width=1024, depth=base_global_buffer_size // 1024)\n",
    "# area_budget = (mac_area * 128 * 128 + base_local_buffer.get_area()) * 4 + base_global_buffer.get_area()\n",
    "\n",
    "# TARGET_TECH_NODE = \"4nm\"\n",
    "# adder = AladdinAdder(technology=TARGET_TECH_NODE, width=16)\n",
    "# multiplier = AladdinMultiplier(technology=TARGET_TECH_NODE, width=8)\n",
    "# mac_area = adder.get_area() + multiplier.get_area()\n",
    "# base_local_buffer_size = 1 * 1024 * 1024 * 8\n",
    "# base_local_buffer = CactiSRAM(technology=TARGET_TECH_NODE, width=128, depth=base_local_buffer_size // 128)\n",
    "# base_global_buffer_size = 16.5 * 1024 * 1024 * 8\n",
    "# base_global_buffer = CactiSRAM(technology=TARGET_TECH_NODE, width=1024, depth=base_global_buffer_size // 1024)\n",
    "# area_budget = (mac_area * 512 * 1024 + base_local_buffer.get_area()) * 4 + base_global_buffer.get_area()\n",
    "\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "\n",
    "objective = lambda df: df['metric_Latency']# * df['metric_Energy']\n",
    "\n",
    "def get_fused_mappings(\n",
    "        spec: Specification,\n",
    "        n_pes,\n",
    "        pe_x,\n",
    "        pe_y,\n",
    "        tagger=None, \n",
    "        # fuse=True,\n",
    "        parameterization=\"\",\n",
    "        return_mappings=False,\n",
    "        max_latency: float = None\n",
    "    ) -> PartialMappings:\n",
    "    print(f'Running parameterization {parameterization}')\n",
    "    cachekey = (n_pes, pe_x, pe_y, parameterization)\n",
    "    fname = parameterization + \" \" + hashlib.md5(str(cachekey).encode()).hexdigest()\n",
    "    if os.path.exists(f\"cache2/{fname}.pkl\") and parameterization != \"FlashAttention B\":\n",
    "        print(f\"Loading from cache2: {fname}\")\n",
    "        mappings = pickle.load(open(f\"cache2/{fname}.pkl\", \"rb\"))\n",
    "        if return_mappings:\n",
    "            return mappings\n",
    "        return objective(mappings.data).min(), mappings\n",
    "    spec = copy.deepcopy(spec)\n",
    "    # local_buffer: Memory = spec.architecture.nodes[\"LocalBuffer\"]\n",
    "    # local_buffer.attributes.size = local_buffer_model.width * local_buffer_model.depth\n",
    "    # local_buffer.spatial.fanout[\"X\"] = n_pes\n",
    "    global_buffer: Memory = spec.architecture.nodes[\"GlobalBuffer\"]\n",
    "    main_memory: Memory = spec.architecture.nodes[\"MainMemory\"]\n",
    "    if parameterization == \"Unfused\":\n",
    "        main_memory.constraints.storage.keep = \"All()\"\n",
    "    elif parameterization == \"FlashAttention B\":\n",
    "        main_memory.constraints.storage.keep = \"All() - (Q | K | QK | QK_softmax)\"# - QK_softmax\"# - Q - K - V - I\"\n",
    "        main_memory.constraints.storage.bypass = \"Q | K | QK | QK_softmax\"#Q | K | V | I\"# | QK | FFA\"\n",
    "    elif parameterization == \"FlashAttention A\":\n",
    "        main_memory.constraints.storage.keep = \"All() - (QK | QK_softmax)\"# - QK_softmax\"# - Q - K - V - I\"\n",
    "        main_memory.constraints.storage.bypass = \"QK | QK_softmax\"#Q | K | V | I\"# | QK | FFA\"\n",
    "    elif parameterization == \"FFM\":\n",
    "        main_memory.constraints.storage.keep = \"~Intermediates() - (I | Q | K | V | QK | QK_softmax)\" #\"# | AV | Z \"\n",
    "        # main_memory.constraints.storage.bypass = \"I | Q | K | V | QK | QK_softmax\"#Q | K | V | I\"# | QK | FFA\"\n",
    "        pass\n",
    "    elif parameterization == \"Weight-Stationary\":\n",
    "        main_memory.constraints.storage.keep = \"~Intermediates()\" #\"# | AV | Z \"\n",
    "        global_buffer.constraints.dataflow.storage_orders.append([\"weight\", \"input\", \"output\"])\n",
    "    elif parameterization == \"Input-Stationary\":\n",
    "        global_buffer.constraints.dataflow.storage_orders.append([\"input\", \"output\", \"weight\"])\n",
    "    elif parameterization == \"Output-Stationary\":\n",
    "        global_buffer.constraints.dataflow.storage_orders.append([\"output\", \"weight\", \"input\"])\n",
    "    elif parameterization == \"Fixed-Dataflow\":\n",
    "        main_memory.constraints.storage.keep = \"~Intermediates() | weight\"\n",
    "        global_buffer.constraints.dataflow.storage_orders.append([\n",
    "            \"MainMemory.tensors() & weight\", \n",
    "            \"MainMemory.tensors() & input\", \n",
    "            \"MainMemory.tensors() & output\", \n",
    "            \"~MainMemory.tensors() & weight\", \n",
    "            \"~MainMemory.tensors() & input\", \n",
    "            \"~MainMemory.tensors() & output\",\n",
    "        ])\n",
    "    else:\n",
    "        assert False, f\"Parameterization {parameterization} not supported\"\n",
    "        \n",
    "    register: Memory = spec.architecture.nodes[\"Register\"]\n",
    "    register.spatial.fanout[\"X\"] = pe_x\n",
    "    register.spatial.fanout[\"Y\"] = pe_y\n",
    "    register.spatial.fanout[\"Z\"] = n_pes\n",
    "    \n",
    "    spec.estimate_energy_area()\n",
    "    flattened_architecture = spec.get_flattened_architecture()\n",
    "    t0 = time.time()\n",
    "    sims, decompress_data = get_sims(spec, flattened_architecture, tagger=tagger, metrics=Metrics.LATENCY | Metrics.ENERGY) # metrics=Metrics.ENERGY | # | Metrics.PER_COMPONENT_ENERGY)\n",
    "    pmapping_time = time.time() - t0\n",
    "    total_pmappings = sum(p.mappings.n_pmappings for v in sims.values() for p in v)\n",
    "    n_pareto_optimal_mappings = sum(len(p.mappings.data) for v in sims.values() for p in v)\n",
    "    print(f'Took {pmapping_time:.2f} seconds to generate {total_pmappings} partial mappings ({total_pmappings / pmapping_time:.2f} per second). {n_pareto_optimal_mappings} pareto optimal mappings ({n_pareto_optimal_mappings / total_pmappings*100:.2f}% of total).')\n",
    "    t0 = time.time()\n",
    "    mappings = join_sims(sims, spec, flattened_architecture)\n",
    "    join_time = time.time() - t0\n",
    "    mappings.decompress(decompress_data)\n",
    "    print(f\"Pmappings: {pmapping_time:.2f}. Joining: {join_time:.2f}. Total Pmappings: {total_pmappings}. Total mappings: {mappings.n_pmappings}. Time per pmapping: {pmapping_time / total_pmappings:.2e}\")\n",
    "    pickle.dump(mappings, open(f\"cache2/{fname}.pkl\", \"wb\"))\n",
    "    if return_mappings:\n",
    "        return mappings\n",
    "    return objective(mappings.data).min(), mappings\n",
    "\n",
    "# print(f'Overall area budget: {area_budget * 1e6} mm^2')\n",
    "\n",
    "parameterization2edp = {}\n",
    "parameterization2mappings = {}\n",
    "\n",
    "parameterizations = [\n",
    "    \"Unfused\",\n",
    "    \"Fixed-Dataflow\",\n",
    "    \"FlashAttention A\",\n",
    "    \"FlashAttention B\",\n",
    "    \"FFM\"\n",
    "]#, \"Unfused\"] # \"FFM\", \"Unfused\", \"FlashAttention\"]#, \"FlashAttention\", \"Unfused\"]\n",
    "\n",
    "# TARGET_TECH_NODE = \"4nm\"\n",
    "# adder = AladdinAdder(technology=TARGET_TECH_NODE, width=16)\n",
    "# multiplier = AladdinMultiplier(technology=TARGET_TECH_NODE, width=8)\n",
    "# mac_area = adder.get_area() + multiplier.get_area()\n",
    "\n",
    "# # glb_size = 512 * 1024 * 1024 * 8\n",
    "# # glb = CactiSRAM(technology=TARGET_TECH_NODE, width=1024, depth=glb_size // 1024)\n",
    "# llb_size = 1 * 1024 * 1024 * 8\n",
    "# llb = CactiSRAM(technology=TARGET_TECH_NODE, width=128, depth=llb_size // 128)\n",
    "\n",
    "def binary_search_glb_size(area_remaining):\n",
    "    print(f\"Area remaining: {area_remaining}\")\n",
    "    # Binary search to nearest 1MB\n",
    "    low = 1\n",
    "    high = 1024\n",
    "    while low < high:\n",
    "        mid = round((low + high) / 2)\n",
    "        size = mid * 1024 * 1024 * 8\n",
    "        glb = CactiSRAM(technology=TARGET_TECH_NODE, width=1024, depth=size // 1024)\n",
    "        if glb.get_area() > area_remaining:\n",
    "            if mid == high:\n",
    "                break\n",
    "            print(f\"High {high} -> {mid}\")\n",
    "            high = mid\n",
    "        else:\n",
    "            if mid == low:\n",
    "                break\n",
    "            print(f\"Low {low} -> {mid}\")\n",
    "            low = mid\n",
    "    return low * 1024 * 1024 * 8\n",
    "\n",
    "mac_x, mac_y = 128, 128\n",
    "# for mac_x, mac_y in [(256, 256), (1024, 1024), (1024, 512), (512, 512), (512, 256)]:\n",
    "# for n_pes in [256, 1, 2, 4, 8, 16, 32, 64, 128, 256]:#2, 4, 8, 16, 32]:\n",
    "for n_pes in [256, 64]:#, 32, 16, 8, 4, 2, 1]:\n",
    "    # total_mac_area = mac_area * mac_x * mac_y\n",
    "    # area_remaining = area_budget - n_pes * (llb.get_area() + total_mac_area)\n",
    "    # if area_remaining < 0:\n",
    "    #     continue\n",
    "    # glb_size = binary_search_glb_size(area_remaining)\n",
    "    # glb = CactiSRAM(technology=TARGET_TECH_NODE, width=1024, depth=glb_size // 1024)\n",
    "    # glb_size = 128 * 1024 * 1024 * 8\n",
    "    # glb = CactiSRAM(technology=TARGET_TECH_NODE, width=1024, depth=glb_size // 1024)\n",
    "    max_latency = None\n",
    "    \n",
    "    # glb_MB = glb_size // 1024 // 1024 // 8\n",
    "    # llb_MB = llb_size // 1024 // 1024 // 8\n",
    "    \n",
    "    print(f\"\\n\\n\")\n",
    "    print(f\"=\" * 100)\n",
    "    print(f\"MAC dims: {mac_x}x{mac_y}\")\n",
    "    print(f\"=\" * 100)\n",
    "\n",
    "    for parameterization in parameterizations: # \"fuse\"\n",
    "        while True:\n",
    "            try:\n",
    "                x, mappings = get_fused_mappings(\n",
    "                    spec,\n",
    "                    n_pes,\n",
    "                    mac_x,\n",
    "                    mac_y,\n",
    "                    parameterization=parameterization,\n",
    "                    max_latency=max_latency if parameterization != \"FlashAttention B\" else None\n",
    "                )\n",
    "                break\n",
    "            except TypeError as e:\n",
    "                max_latency *= 2\n",
    "                print(f\"Error: {e}\")\n",
    "        if \"Fixed-Dataflow\" not in parameterization:\n",
    "            max_latency = x\n",
    "        if x != 0:\n",
    "            parameterization2edp[f\"{parameterization} {n_pes} {mac_x}x{mac_y}\"] = x\n",
    "            parameterization2mappings[f\"{parameterization} {n_pes} {mac_x}x{mac_y}\"] = mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m plt.style.use(\u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "assert False\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "# plt.rcParams.update({'font.size': 28})\n",
    "\n",
    "def plot_default_formatting(ax, grid_axis='both'):\n",
    "    ax.tick_params(axis='both', which='major')#, labelsize=20)\n",
    "    ax.tick_params(axis='both', which='minor')#, labelsize=20)\n",
    "    legend = ax.legend()\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    legend.get_frame().set_edgecolor('black')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('black')\n",
    "    if ax.get_legend() is None:\n",
    "        legend = ax.legend(fontsize=24, ncol=2)\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(axis=grid_axis, which='major', linestyle='-', linewidth='0.3', color='gray')\n",
    "    ax.grid(axis=grid_axis, which='minor', linestyle='--', linewidth='0.1', color='lightgray')\n",
    "    \n",
    "\n",
    "def make_bar_chart(\n",
    "    data,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    "    y_scale,\n",
    "    output_file=None,\n",
    "    normalize: bool = False,\n",
    "    ylim=(None, None),\n",
    "    xlim=(None, None),\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a bar chart from the given data and save it as a PDF.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    if isinstance(data, dict) and isinstance(next(iter(data.values())), dict):\n",
    "        bar_width = 0.8 / len(data)\n",
    "        keys = list(next(iter(data.values())).keys())\n",
    "        x = range(len(keys))\n",
    "        first = next(iter(data.values()))\n",
    "            \n",
    "        for i, (label, values) in enumerate(data.items()):\n",
    "            bar_positions = [pos + i * bar_width for pos in x]\n",
    "            to_plot = values\n",
    "            if normalize:\n",
    "                to_plot = {k: v / first[k] for k, v in values.items()}\n",
    "            bars = plt.bar(bar_positions, to_plot.values(), width=bar_width, label=label)\n",
    "        plt.xticks([pos + (len(data) - 1) * bar_width / 2 for pos in x], keys)\n",
    "        plt.legend(loc='upper right', fontsize=10)\n",
    "    else:\n",
    "        keys = list(data.keys())\n",
    "        bars = plt.bar(keys, data.values())\n",
    "\n",
    "    # Set logarithmic scale for Y-axis if specified\n",
    "    if y_scale == 'log':\n",
    "        plt.yscale('log')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlim(xlim)\n",
    "\n",
    "    # Rotate X-axis labels vertically\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plot_default_formatting(plt.gca(), grid_axis='y')\n",
    "    \n",
    "    if output_file is not None:\n",
    "        with open(output_file, 'wb') as f:\n",
    "            plt.savefig(f, format='pdf', bbox_inches='tight')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "print()\n",
    "make_bar_chart(\n",
    "    parameterization2edp,\n",
    "    title=None,\n",
    "    xlabel=None,\n",
    "    ylabel=\"EDP\",\n",
    "    y_scale='linear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # glb_MB = 128\n",
    "# # sram_MB = 4\n",
    "# # parameterization = \"\"\n",
    "\n",
    "# # cur_area_budget = area_budget\n",
    "# # glb_size = glb_MB * 1024 * 1024 * 8\n",
    "# # glb = CactiSRAM(technology=\"7nm\", width=1024, depth=glb_size // 1024)\n",
    "# # cur_area_budget -= glb.get_area()\n",
    "# # # for sram_MB in [0.25, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4]:\n",
    "# # sram_size = sram_MB * 1024 * 1024 * 8\n",
    "# # llb = CactiSRAM(technology=\"7nm\", width=128, depth=sram_size // 128)\n",
    "# # remaining_area = cur_area_budget / 4 - llb.get_area() # Per-MXU\n",
    "# # mac_dims = int((remaining_area / mac_area) ** 0.5)\n",
    "# # print(f\"Global buffer: {glb_MB} MB, Local buffer: {sram_MB} MB, MAC dims: {mac_dims}x{mac_dims}\")\n",
    "# # print(f'GLB read energy: {glb.read()}. LLB read energy: {llb.read()}')\n",
    "\n",
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_per_tensor_size, get_num_computes\n",
    "for tensor, size in sorted(get_per_tensor_size(spec).items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{tensor}: {size}\")\n",
    "print(f\"Number of computes: {get_num_computes(spec)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_rank_variable_bounds_for_all_einsums\n",
    "\n",
    "parameterization2latencycols: list[dict[str, float]] = []\n",
    "for p, mappings in parameterization2mappings.items():\n",
    "    mappings._data = mappings.data.sort_values(by=\"metric_Latency\", ascending=True)\n",
    "    rank_variable_bounds = get_rank_variable_bounds_for_all_einsums(spec)\n",
    "\n",
    "    row = {\n",
    "        \"Parameterization\": p,\n",
    "    }\n",
    "    for col in mappings.data.columns:\n",
    "        print(f'{col}: {mappings.data.iloc[0][col]}')\n",
    "        # if \"Latency\" in col:\n",
    "        # if \"metric_Latency\" in col:\n",
    "        if \"Latency\" in col:\n",
    "        # if \"metric_Energy\" in col:\n",
    "            row[col] = mappings.data.iloc[0][col]\n",
    "    parameterization2latencycols.append(row)\n",
    "\n",
    "    # from fastfusion.mapper.FFM.visualization import make_mapping\n",
    "    # from IPython.display import SVG\n",
    "    # newmapping = make_mapping(mappings.data.iloc[0], spec.workload.einsum_names, get_rank_variable_bounds_for_all_einsums(spec))\n",
    "    # for col in mappings.data.columns:\n",
    "    #     print(f'{col}: {mappings.data.iloc[0][col]}')\n",
    "\n",
    "    # display(SVG(newmapping.render()))\n",
    "    \n",
    "from pandas import DataFrame\n",
    "df = DataFrame(parameterization2latencycols)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "df\n",
    "    \n",
    "# {'n1'}-1 || [GlobalBuffer] T1 sz 0 above 1\n",
    "# TODO: Re-add -1 to the mapper one eisnum freenig\n",
    "# compatibility2sims['Matmul1'][\"{'n1'}-1 || [GlobalBuffer] T1 sz 0 above 1\"]\n",
    "# Above 1: 8192\n",
    "# Above 2: 8321\n",
    "# compatibility2sims['Matmul2'][\"{'n1'}-1 || [GlobalBuffer] T1 sz 0 above 1, [GlobalBuffer] T2 sz 0 above 0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_rank_variable_bounds_for_all_einsums\n",
    "\n",
    "mac_dims = int((((area_budget - glb.get_area()) / 4 - llb.get_area()) / mac_area)** 0.5)\n",
    "mappings = list(parameterization2mappings.values())[0]\n",
    "mappings._data = mappings.data.sort_values(by=\"metric_Latency\", ascending=True).head()\n",
    "rank_variable_bounds = get_rank_variable_bounds_for_all_einsums(spec)\n",
    "from fastfusion.mapper.FFM.visualization import make_mapping\n",
    "from IPython.display import SVG\n",
    "newmapping = make_mapping(mappings.data.iloc[0], spec.workload.einsum_names, get_rank_variable_bounds_for_all_einsums(spec))\n",
    "a = {}\n",
    "for col in mappings.data.columns:\n",
    "    print(f'{col}: {mappings.data.iloc[0][col]}')\n",
    "    if \"Latency\" in col:\n",
    "        a[col] = mappings.data.iloc[0][col]\n",
    "display(SVG(newmapping.render()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False\n",
    "\n",
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_rank_variable_bounds_for_all_einsums\n",
    "\n",
    "sram_size = 0.5 * 1024 * 1024 * 8\n",
    "llb = CactiSRAM(technology=\"7nm\", width=128, depth=sram_size // 128)\n",
    "mac_dims = int((((area_budget - glb.get_area()) / 4 - llb.get_area()) / mac_area)** 0.5)\n",
    "mappings = get_fused_mappings(\n",
    "    spec,\n",
    "    mac_dims,\n",
    "    llb,\n",
    "    glb,\n",
    "    return_mappings=True,\n",
    "    parameterization=\"FFM\"\n",
    ")\n",
    "mappings._data = mappings.data.sort_values(by=\"metric_Latency\", ascending=True).head()\n",
    "rank_variable_bounds = get_rank_variable_bounds_for_all_einsums(spec)\n",
    "from fastfusion.mapper.FFM.visualization import make_mapping\n",
    "from IPython.display import SVG\n",
    "newmapping = make_mapping(mappings.data.iloc[0], spec.workload.einsum_names, get_rank_variable_bounds_for_all_einsums(spec))\n",
    "b = {}\n",
    "for col in mappings.data.columns:\n",
    "    print(f'{col}: {mappings.data.iloc[0][col]}')\n",
    "    if \"Latency\" in col:\n",
    "        b[col] = mappings.data.iloc[0][col]\n",
    "display(SVG(newmapping.render()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([a, b])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
