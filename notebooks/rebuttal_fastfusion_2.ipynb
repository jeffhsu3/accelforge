{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO        Loading yaml file architecture/four_level.arch.yaml\n",
      "INFO        Found top key variables in architecture/four_level.arch.yaml\n",
      "INFO        Found top key architecture in architecture/four_level.arch.yaml\n",
      "INFO        Found top key component_classes in architecture/four_level.arch.yaml\n",
      "INFO        Loading yaml file workloads/mha_full.workload.yaml\n",
      "INFO        Found top key workload in workloads/mha_full.workload.yaml\n",
      "INFO        Loading yaml file workloads/mha_full.renames.yaml\n",
      "INFO        Found top key renames in workloads/mha_full.renames.yaml\n",
      "WARNING     Loading configuration file from /root/.config/fastfusion/config.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTE ENERGY / 8 ????????????\n",
      "COMPUTE ENERGY / 8 ????????????\n",
      "COMPUTE ENERGY / 8 ????????????\n",
      "COMPUTE ENERGY / 8 ????????????\n",
      "COMPUTE ENERGY / 8 ????????????\n",
      "Overall area budget: 39.657475727216784 mm^2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO        Calculated \"614e9 * 8 / 1.05e9\" = 4678.0952380952385.\n",
      "INFO        Calculated \"0.84e-12 / 8\" = 1.05e-13.\n",
      "INFO        Calculated \"0.5\" = 0.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global buffer: 128 MB, Local buffer: 4 MB, MAC dims: 128x128\n",
      "GLB read energy: 1.6504301873093021e-09. LLB read energy: 4.2315915388286673e-11\n",
      "By default metrics optimizes for energy and latency.We should change to just energy or just latency at some point.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating storage and loop choices for Einsum I: 6it [00:00, 83.13it/s]\n",
      "Generating Pmappings for I:  17%|█▋        | 1/6 [00:00<00:02,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 65 skipped (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Pmappings for I:  33%|███▎      | 2/6 [00:00<00:01,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 54 skipped (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Pmappings for I:  50%|█████     | 3/6 [00:01<00:01,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 65 skipped (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Pmappings for I:  67%|██████▋   | 4/6 [00:01<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 65 skipped (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Pmappings for I:  83%|████████▎ | 5/6 [00:02<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 54 skipped (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Pmappings for I: 100%|██████████| 6/6 [00:02<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 54 skipped (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating storage and loop choices for Einsum V: 8it [00:00, 97.97it/s]\n",
      "Generating Pmappings for V:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 / 200 skipped (4.50%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Pmappings for V:  25%|██▌       | 2/8 [00:08<00:23,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Compatibility(loops=(Loop({'b'}, 0, False),), storage={Reservation('I', 1, 'GlobalBuffer', 0)}), tags=Tags(({})) because it is not in any tensor2boundless_compatibilities\n",
      "48 / 432 skipped (11.11%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Pmappings for V:  50%|█████     | 4/8 [00:11<00:09,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Compatibility(loops=(Loop({'b'}, 0, False),), storage={Reservation('I', 1, 'GlobalBuffer', 0)}), tags=Tags(({})) because it is not in any tensor2boundless_compatibilities\n",
      "117 / 117 skipped (100.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Pmappings for V:  50%|█████     | 4/8 [00:50<00:50, 12.74s/it]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Skipped everyone! Compatibility: Compatibility(loops=(Loop({'b'}, 0, False), Loop({'m'}, 0, False), Loop({'b'}, 0, False), Loop({'d'}, 0, False)), storage={Reservation('I', 4, 'GlobalBuffer', 0), Reservation('V', 2, 'GlobalBuffer', 0)}), tags=Tags(({}))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 152\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mGLB read energy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglb.read()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. LLB read energy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllb.read()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m parameterization \u001b[38;5;129;01min\u001b[39;00m parameterizations: \u001b[38;5;66;03m# \"fuse\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     x = \u001b[43mget_fused_mappings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmac_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mllb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mglb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameterization\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameterization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x != \u001b[32m0\u001b[39m:\n\u001b[32m    160\u001b[39m         parameterization2edp[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparameterization\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglb_MB\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mMB \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msram_MB\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mMB \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmac_dims\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmac_dims\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = x\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mget_fused_mappings\u001b[39m\u001b[34m(spec, n_pes, local_buffer_model, global_buffer_model, tagger, parameterization, return_mappings)\u001b[39m\n\u001b[32m    113\u001b[39m flattened_architecture = spec.get_flattened_architecture()\n\u001b[32m    114\u001b[39m t0 = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m sims, decompress_data = \u001b[43mget_sims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflattened_architecture\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLATENCY\u001b[49m\u001b[43m \u001b[49m\u001b[43m|\u001b[49m\u001b[43m \u001b[49m\u001b[43mMetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mENERGY\u001b[49m\u001b[43m \u001b[49m\u001b[43m|\u001b[49m\u001b[43m \u001b[49m\u001b[43mMetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPER_COMPONENT_ENERGY\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# metrics=Metrics.ENERGY | # | Metrics.PER_COMPONENT_ENERGY)\u001b[39;00m\n\u001b[32m    116\u001b[39m pmapping_time = time.time() - t0\n\u001b[32m    117\u001b[39m total_pmappings = \u001b[38;5;28msum\u001b[39m(p.mappings.n_pmappings \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m sims.values() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m v)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/fastfusion/fastfusion/mapper/FFM/exploration/mapper_multi_einsum.py:110\u001b[39m, in \u001b[36mget_sims\u001b[39m\u001b[34m(spec, flattened_arch, tagger, metrics, einsum_names, except_from_imperfect)\u001b[39m\n\u001b[32m     93\u001b[39m relevant_tensor2boundless_compatibilities = {\n\u001b[32m     94\u001b[39m     t: \u001b[38;5;28mset\u001b[39m(\n\u001b[32m     95\u001b[39m         c.clear_loop_bounds()\n\u001b[32m   (...)\u001b[39m\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m relevant_tensor2compatibilties\n\u001b[32m     99\u001b[39m }\n\u001b[32m    101\u001b[39m jobs = get_single_einsum_jobs(\n\u001b[32m    102\u001b[39m     spec=spec,\n\u001b[32m    103\u001b[39m     einsum_name=einsum_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    108\u001b[39m     tensor2boundless_compatibilities=relevant_tensor2boundless_compatibilities,\n\u001b[32m    109\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m einsum_name, new_sims, decompress_data, job_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGenerating Pmappings for \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43meinsum_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_as\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenerator_unordered\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    115\u001b[39m     cur_sims.extend(new_sims)\n\u001b[32m    116\u001b[39m     grouped_decompress_data.register_decompress_data(\n\u001b[32m    117\u001b[39m         einsum_name,\n\u001b[32m    118\u001b[39m         job_id,\n\u001b[32m    119\u001b[39m         decompress_data,\n\u001b[32m    120\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/fastfusion/fastfusion/util/util.py:125\u001b[39m, in \u001b[36mparallel\u001b[39m\u001b[34m(jobs, n_jobs, one_job_if_debugging, pbar, return_as, chunk, delete_job_after)\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pbar:\n\u001b[32m    124\u001b[39m         jobs = tqdm(jobs, total=\u001b[38;5;28mlen\u001b[39m(jobs), desc=pbar, leave=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mj\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mj\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mj\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m jobs]\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# We were getting a lot of the runtime in parallelizing overhead. What this\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# does is chunks jobs into larger groups. The last n_jobs groups are 1 job,\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# the previous n_jobs groups are 2 jobs, the previous n_jobs groups are 4\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# jobs, and so on. Jobs get smaller near the end to reduce the impact of\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# long pole jobs.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunk \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m delete_job_after:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/fastfusion/fastfusion/mapper/FFM/exploration/mapper_one_einsum/mapper_one_einsum.py:666\u001b[39m, in \u001b[36m_per_proc_compatibility2sim\u001b[39m\u001b[34m(mapping, constraints, spec, rank_variable_bounds, intermediate_tensors, flattened_arch, einsum_name, metrics, job_id, tagger, tensor2compatibilties, tensor2boundless_compatibilities)\u001b[39m\n\u001b[32m    662\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m einsum_name, [], \u001b[38;5;28;01mNone\u001b[39;00m, job_id\n\u001b[32m    664\u001b[39m sims = make_sims(compatibility, result, rank_variable_bounds, intermediate_tensors, tagger=tagger, total_pmappings=total_pmappings, tensor2compatibilties=tensor2compatibilties)\n\u001b[32m    665\u001b[39m decompress_data = PartialMappings.compress_paretos(\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m     einsum_name, \n\u001b[32m    667\u001b[39m     [s.mappings \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sims],\n\u001b[32m    668\u001b[39m     job_id=job_id,\n\u001b[32m    669\u001b[39m     extra_data={MAPPING_COLUMN: mapping}\n\u001b[32m    670\u001b[39m )\n\u001b[32m    671\u001b[39m gc.collect()\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m einsum_name, sims, decompress_data, job_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/fastfusion/fastfusion/mapper/FFM/exploration/mapper_one_einsum/mapper_one_einsum.py:629\u001b[39m, in \u001b[36mmake_sims\u001b[39m\u001b[34m(compatibility, explored_results, rank_variable_bounds, intermediate_tensors, tagger, total_pmappings, tensor2compatibilties)\u001b[39m\n\u001b[32m    625\u001b[39m         sims.append(equivalent_sim)\n\u001b[32m    627\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_skipped\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m skipped (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_skipped\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sims\n",
      "\u001b[31mAssertionError\u001b[39m: Skipped everyone! Compatibility: Compatibility(loops=(Loop({'b'}, 0, False), Loop({'m'}, 0, False), Loop({'b'}, 0, False), Loop({'d'}, 0, False)), storage={Reservation('I', 4, 'GlobalBuffer', 0), Reservation('V', 2, 'GlobalBuffer', 0)}), tags=Tags(({}))"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "from hwcomponents_cacti import CactiSRAM\n",
    "from hwcomponents_library import AladdinAdder, AladdinMultiplier\n",
    "\n",
    "from fastfusion.frontend.architecture import Memory\n",
    "from fastfusion.frontend.specification import Specification\n",
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_sims\n",
    "from fastfusion.mapper.simanneal.wrappers import join_sims\n",
    "\n",
    "import copy\n",
    "import time\n",
    "from fastfusion import Specification\n",
    "from fastfusion.mapper.FFM.exploration.metrics import Metrics\n",
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_sims\n",
    "from fastfusion.mapper.FFM.joining.sim import SIM\n",
    "from fastfusion.mapper.FFM.joining.simexplore import join_sims\n",
    "import fastfusion.mapper.FFM.exploration.mapper_one_einsum as mapper_one_einsum\n",
    "\n",
    "from fastfusion.mapper.FFM.exploration.mapping_filter_tags.ffmt import get_ffmt_tag\n",
    "from fastfusion.mapper.FFM.exploration.mapping_filter_tags.onesplit import get_one_split_tag\n",
    "from fastfusion.mapper.FFM.pareto import PartialMappings\n",
    "\n",
    "# TODO: area is an alias for get_area\n",
    "# TODO: Separate energy and area\n",
    "# TODO: Move scaling into main hwcomponents repo\n",
    "# TODO: Function that just returns the hwcomponents component\n",
    "# TODO: Is all the initial right consolidation necessary?\n",
    "# TODO: Datawidth calculation for energy\n",
    "\n",
    "# TODO: Reference specific tensor names in constraints, even if those tensors are not in\n",
    "# a particular Einsum. Also have the error mrssages for parsing errors list which Einsum\n",
    "# failed. Einsums that aren't in the tensor should resolve to NotInThisEinsum(), which =\n",
    "# nothing.\n",
    "\n",
    "# TODO: Make a setting for the below two in the spec\n",
    "# TODO: Generate pmappings one Einsum at a time. Once we've made compatibility, check it\n",
    "# against the previously-generated compatibilities and stop if there's no match.\n",
    "# TODO: Once the previous is done, also add a forward check. Once the compatibilities of\n",
    "# a particular Einsum are generated, we can immediately check the previous Einsums.\n",
    "# TODO: Make the mapping return an object that supports union operators and stuff\n",
    "\n",
    "spec = Specification.from_yaml(\n",
    "    f\"architecture/four_level.arch.yaml\",\n",
    "    \"workloads/mha_full.workload.yaml\",\n",
    "    \"workloads/mha_full.renames.yaml\",\n",
    ")\n",
    "\n",
    "adder = AladdinAdder(technology=\"7nm\", width=16)\n",
    "multiplier = AladdinMultiplier(technology=\"7nm\", width=8)\n",
    "mac_area = adder.get_area() + multiplier.get_area()\n",
    "\n",
    "base_local_buffer_size = 4 * 1024 * 1024 * 8\n",
    "base_local_buffer = CactiSRAM(technology=\"7nm\", width=128, depth=base_local_buffer_size // 128)\n",
    "base_global_buffer_size = 128 * 1024 * 1024 * 8\n",
    "base_global_buffer = CactiSRAM(technology=\"7nm\", width=1024, depth=base_global_buffer_size // 1024)\n",
    "\n",
    "area_budget = (mac_area * 128 * 128 + base_local_buffer.get_area()) * 4 + base_global_buffer.get_area()\n",
    "\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "print(f\"COMPUTE ENERGY / 8 ????????????\")\n",
    "\n",
    "def get_fused_mappings(\n",
    "        spec: Specification, \n",
    "        n_pes,\n",
    "        local_buffer_model,\n",
    "        global_buffer_model,\n",
    "        tagger=None, \n",
    "        # fuse=True,\n",
    "        parameterization=\"\",\n",
    "        return_mappings=False,\n",
    "    ) -> PartialMappings:\n",
    "    cachekey = (n_pes, local_buffer_model.width, local_buffer_model.depth, global_buffer_model.width, global_buffer_model.depth, parameterization)\n",
    "    fname = parameterization + \" \" + hashlib.md5(str(cachekey).encode()).hexdigest()\n",
    "    if os.path.exists(f\"cache/{fname}.pkl\"):\n",
    "        print(f\"Loading from cache: {fname}\")\n",
    "        mappings = pickle.load(open(f\"cache/{fname}.pkl\", \"rb\"))\n",
    "        if return_mappings:\n",
    "            return mappings\n",
    "        return (mappings.data[\"metric_Energy\"] * mappings.data[\"metric_Latency\"]).min()\n",
    "    spec = copy.deepcopy(spec)\n",
    "    local_buffer: Memory = spec.architecture.nodes[\"LocalBuffer\"]\n",
    "    local_buffer.attributes.size = local_buffer_model.width * local_buffer_model.depth\n",
    "    global_buffer: Memory = spec.architecture.nodes[\"GlobalBuffer\"]\n",
    "    global_buffer.attributes.size = global_buffer_model.width * global_buffer_model.depth\n",
    "    for target, model in [(local_buffer, local_buffer_model), (global_buffer, global_buffer_model)]:\n",
    "        target.actions[\"read\"].arguments.energy = model.read() / model.width\n",
    "        target.actions[\"write\"].arguments.energy = model.write() / model.width\n",
    "    main_memory: Memory = spec.architecture.nodes[\"MainMemory\"]\n",
    "    if parameterization == \"Unfused\":\n",
    "        main_memory.constraints.storage.keep = \"All()\"\n",
    "    elif parameterization == \"FlashAttention\":\n",
    "        main_memory.constraints.storage.keep = \"All() - QK - Q - K - V - I\"\n",
    "        main_memory.constraints.storage.bypass = \"QK | Q | K | V | I\"\n",
    "    elif parameterization == \"Fuse I\":\n",
    "        main_memory.constraints.storage.keep = \"All() - I\"\n",
    "        main_memory.constraints.storage.bypass = \"I\"\n",
    "    elif parameterization == \"FFM\":\n",
    "        main_memory.constraints.storage.keep = \"~Intermediates()\"# - I - Q - K - V\"# | AV | Z \"\n",
    "        main_memory.constraints.storage.bypass = \"Q | K | V | I\"#Q | K | V | I\"# | QK | FFA\"\n",
    "        pass\n",
    "    else:\n",
    "        assert False, f\"Parameterization {parameterization} not supported\"\n",
    "    register: Memory = spec.architecture.nodes[\"Register\"]\n",
    "    register.spatial.fanout[\"X\"] = n_pes\n",
    "    register.spatial.fanout[\"Y\"] = n_pes\n",
    "    \n",
    "    spec.estimate_energy_area()\n",
    "    flattened_architecture = spec.get_flattened_architecture()\n",
    "    t0 = time.time()\n",
    "    sims, decompress_data = get_sims(spec, flattened_architecture, tagger=tagger, metrics=Metrics.LATENCY | Metrics.ENERGY | Metrics.PER_COMPONENT_ENERGY) # metrics=Metrics.ENERGY | # | Metrics.PER_COMPONENT_ENERGY)\n",
    "    pmapping_time = time.time() - t0\n",
    "    total_pmappings = sum(p.mappings.n_pmappings for v in sims.values() for p in v)\n",
    "    n_pareto_optimal_mappings = sum(len(p.mappings.data) for v in sims.values() for p in v)\n",
    "    print(f'Took {pmapping_time:.2f} seconds to generate {total_pmappings} partial mappings ({total_pmappings / pmapping_time:.2f} per second). {n_pareto_optimal_mappings} pareto optimal mappings ({n_pareto_optimal_mappings / total_pmappings*100:.2f}% of total).')\n",
    "    t0 = time.time()\n",
    "    mappings = join_sims(sims, spec, flattened_architecture)\n",
    "    join_time = time.time() - t0\n",
    "    mappings.decompress(decompress_data)\n",
    "    print(f\"Pmappings: {pmapping_time:.2f}. Joining: {join_time:.2f}. Total Pmappings: {total_pmappings}. Total mappings: {mappings.n_pmappings}. Time per pmapping: {pmapping_time / total_pmappings:.2e}\")\n",
    "    pickle.dump(mappings, open(f\"cache/{fname}.pkl\", \"wb\"))\n",
    "    if return_mappings:\n",
    "        return mappings\n",
    "    return (mappings.data[\"metric_Energy\"] * mappings.data[\"metric_Latency\"]).min()\n",
    "\n",
    "print(f'Overall area budget: {area_budget * 1e6} mm^2')\n",
    "\n",
    "parameterization2edp = {}\n",
    "parameterizations = [\"FFM\", \"Unfused\", \"FlashAttention\"]#, \"FlashAttention\", \"Unfused\"]\n",
    "\n",
    "for glb_MB in [128]:#, 64, 32, 16]:#,16]:#16, 32, 64, 128]: # [16, 32, 64, 128]: # 16, 64\n",
    "    cur_area_budget = area_budget\n",
    "    glb_size = glb_MB * 1024 * 1024 * 8\n",
    "    glb = CactiSRAM(technology=\"7nm\", width=1024, depth=glb_size // 1024)\n",
    "    cur_area_budget -= glb.get_area()\n",
    "    # for sram_MB in [0.25, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4]:\n",
    "    for sram_MB in [4]:#, 1, 0.25]:#,0.5]:#[0.5,1,2,4]: # [0.25, 1, 4]: # 0.125, 0.5, 2\n",
    "        sram_size = sram_MB * 1024 * 1024 * 8\n",
    "        llb = CactiSRAM(technology=\"7nm\", width=128, depth=sram_size // 128)\n",
    "        remaining_area = cur_area_budget / 4 - llb.get_area() # Per-MXU\n",
    "        if remaining_area < 0:\n",
    "            break\n",
    "        mac_dims = int((remaining_area / mac_area) ** 0.5)\n",
    "        print(f\"Global buffer: {glb_MB} MB, Local buffer: {sram_MB} MB, MAC dims: {mac_dims}x{mac_dims}\")\n",
    "        print(f'GLB read energy: {glb.read()}. LLB read energy: {llb.read()}')\n",
    "        \n",
    "        for parameterization in parameterizations: # \"fuse\"\n",
    "            x = get_fused_mappings(\n",
    "                spec,\n",
    "                mac_dims,\n",
    "                llb,\n",
    "                glb,\n",
    "                parameterization=parameterization,\n",
    "            )\n",
    "            if x != 0:\n",
    "                parameterization2edp[f\"{parameterization} {glb_MB}MB {sram_MB}MB {mac_dims}x{mac_dims}\"] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "# plt.rcParams.update({'font.size': 28})\n",
    "\n",
    "def plot_default_formatting(ax, grid_axis='both'):\n",
    "    ax.tick_params(axis='both', which='major')#, labelsize=20)\n",
    "    ax.tick_params(axis='both', which='minor')#, labelsize=20)\n",
    "    legend = ax.legend()\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    legend.get_frame().set_edgecolor('black')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('black')\n",
    "    if ax.get_legend() is None:\n",
    "        legend = ax.legend(fontsize=24, ncol=2)\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(axis=grid_axis, which='major', linestyle='-', linewidth='0.3', color='gray')\n",
    "    ax.grid(axis=grid_axis, which='minor', linestyle='--', linewidth='0.1', color='lightgray')\n",
    "    \n",
    "\n",
    "def make_bar_chart(\n",
    "    data,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    "    y_scale,\n",
    "    output_file=None,\n",
    "    normalize: bool = False,\n",
    "    ylim=(None, None),\n",
    "    xlim=(None, None),\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a bar chart from the given data and save it as a PDF.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    if isinstance(data, dict) and isinstance(next(iter(data.values())), dict):\n",
    "        bar_width = 0.8 / len(data)\n",
    "        keys = list(next(iter(data.values())).keys())\n",
    "        x = range(len(keys))\n",
    "        first = next(iter(data.values()))\n",
    "            \n",
    "        for i, (label, values) in enumerate(data.items()):\n",
    "            bar_positions = [pos + i * bar_width for pos in x]\n",
    "            to_plot = values\n",
    "            if normalize:\n",
    "                to_plot = {k: v / first[k] for k, v in values.items()}\n",
    "            bars = plt.bar(bar_positions, to_plot.values(), width=bar_width, label=label)\n",
    "        plt.xticks([pos + (len(data) - 1) * bar_width / 2 for pos in x], keys)\n",
    "        plt.legend(loc='upper right', fontsize=10)\n",
    "    else:\n",
    "        keys = list(data.keys())\n",
    "        bars = plt.bar(keys, data.values())\n",
    "\n",
    "    # Set logarithmic scale for Y-axis if specified\n",
    "    if y_scale == 'log':\n",
    "        plt.yscale('log')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlim(xlim)\n",
    "\n",
    "    # Rotate X-axis labels vertically\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plot_default_formatting(plt.gca(), grid_axis='y')\n",
    "    \n",
    "    if output_file is not None:\n",
    "        with open(output_file, 'wb') as f:\n",
    "            plt.savefig(f, format='pdf', bbox_inches='tight')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "print()\n",
    "make_bar_chart(\n",
    "    parameterization2edp,\n",
    "    title=None,\n",
    "    xlabel=None,\n",
    "    ylabel=\"EDP\",\n",
    "    y_scale='linear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # glb_MB = 128\n",
    "# # sram_MB = 4\n",
    "# # parameterization = \"\"\n",
    "\n",
    "# # cur_area_budget = area_budget\n",
    "# # glb_size = glb_MB * 1024 * 1024 * 8\n",
    "# # glb = CactiSRAM(technology=\"7nm\", width=1024, depth=glb_size // 1024)\n",
    "# # cur_area_budget -= glb.get_area()\n",
    "# # # for sram_MB in [0.25, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4]:\n",
    "# # sram_size = sram_MB * 1024 * 1024 * 8\n",
    "# # llb = CactiSRAM(technology=\"7nm\", width=128, depth=sram_size // 128)\n",
    "# # remaining_area = cur_area_budget / 4 - llb.get_area() # Per-MXU\n",
    "# # mac_dims = int((remaining_area / mac_area) ** 0.5)\n",
    "# # print(f\"Global buffer: {glb_MB} MB, Local buffer: {sram_MB} MB, MAC dims: {mac_dims}x{mac_dims}\")\n",
    "# # print(f'GLB read energy: {glb.read()}. LLB read energy: {llb.read()}')\n",
    "\n",
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_per_tensor_size, get_num_computes\n",
    "for tensor, size in sorted(get_per_tensor_size(spec).items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{tensor}: {size}\")\n",
    "print(f\"Number of computes: {get_num_computes(spec)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_rank_variable_bounds_for_all_einsums\n",
    "\n",
    "parameterization2latencycols: list[dict[str, float]] = []\n",
    "for p in parameterizations:\n",
    "    mappings = get_fused_mappings(\n",
    "        spec,\n",
    "        mac_dims,\n",
    "        llb,\n",
    "        glb,\n",
    "        return_mappings=True,\n",
    "        parameterization=p #\n",
    "    )\n",
    "    mappings._data = mappings.data.sort_values(by=\"metric_Energy\", ascending=True)\n",
    "    rank_variable_bounds = get_rank_variable_bounds_for_all_einsums(spec)\n",
    "\n",
    "    row = {\n",
    "        \"Parameterization\": p,\n",
    "    }\n",
    "    for col in mappings.data.columns:\n",
    "        print(f'{col}: {mappings.data.iloc[0][col]}')\n",
    "        # if \"Latency\" in col:\n",
    "        # if \"metric_Latency\" in col:\n",
    "        # if \"Energy\" in col:\n",
    "        # if \"metric_Energy\" in col:\n",
    "            # row[col] = mappings.data.iloc[0][col]\n",
    "    parameterization2latencycols.append(row)\n",
    "\n",
    "    # from fastfusion.mapper.FFM.visualization import make_mapping\n",
    "    # from IPython.display import SVG\n",
    "    # newmapping = make_mapping(mappings.data.iloc[0], spec.workload.einsum_names, get_rank_variable_bounds_for_all_einsums(spec))\n",
    "    # for col in mappings.data.columns:\n",
    "    #     print(f'{col}: {mappings.data.iloc[0][col]}')\n",
    "\n",
    "    # display(SVG(newmapping.render()))\n",
    "    \n",
    "from pandas import DataFrame\n",
    "df = DataFrame(parameterization2latencycols)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "df\n",
    "    \n",
    "# {'n1'}-1 || [GlobalBuffer] T1 sz 0 above 1\n",
    "# TODO: Re-add -1 to the mapper one eisnum freenig\n",
    "# compatibility2sims['Matmul1'][\"{'n1'}-1 || [GlobalBuffer] T1 sz 0 above 1\"]\n",
    "# Above 1: 8192\n",
    "# Above 2: 8321\n",
    "# compatibility2sims['Matmul2'][\"{'n1'}-1 || [GlobalBuffer] T1 sz 0 above 1, [GlobalBuffer] T2 sz 0 above 0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_rank_variable_bounds_for_all_einsums\n",
    "\n",
    "sram_size = 4 * 1024 * 1024 * 8\n",
    "llb = CactiSRAM(technology=\"7nm\", width=128, depth=sram_size // 128)\n",
    "mac_dims = int((((area_budget - glb.get_area()) / 4 - llb.get_area()) / mac_area)** 0.5)\n",
    "mappings = get_fused_mappings(\n",
    "    spec,\n",
    "    mac_dims,\n",
    "    llb,\n",
    "    glb,\n",
    "    return_mappings=True,\n",
    "    parameterization=\"FFM\"\n",
    ")\n",
    "mappings._data = mappings.data.sort_values(by=\"metric_Latency\", ascending=True).head()\n",
    "rank_variable_bounds = get_rank_variable_bounds_for_all_einsums(spec)\n",
    "from fastfusion.mapper.FFM.visualization import make_mapping\n",
    "from IPython.display import SVG\n",
    "newmapping = make_mapping(mappings.data.iloc[0], spec.workload.einsum_names, get_rank_variable_bounds_for_all_einsums(spec))\n",
    "a = {}\n",
    "for col in mappings.data.columns:\n",
    "    print(f'{col}: {mappings.data.iloc[0][col]}')\n",
    "    if \"Latency\" in col:\n",
    "        a[col] = mappings.data.iloc[0][col]\n",
    "display(SVG(newmapping.render()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_rank_variable_bounds_for_all_einsums\n",
    "\n",
    "sram_size = 0.5 * 1024 * 1024 * 8\n",
    "llb = CactiSRAM(technology=\"7nm\", width=128, depth=sram_size // 128)\n",
    "mac_dims = int((((area_budget - glb.get_area()) / 4 - llb.get_area()) / mac_area)** 0.5)\n",
    "mappings = get_fused_mappings(\n",
    "    spec,\n",
    "    mac_dims,\n",
    "    llb,\n",
    "    glb,\n",
    "    return_mappings=True,\n",
    "    parameterization=\"FFM\"\n",
    ")\n",
    "mappings._data = mappings.data.sort_values(by=\"metric_Latency\", ascending=True).head()\n",
    "rank_variable_bounds = get_rank_variable_bounds_for_all_einsums(spec)\n",
    "from fastfusion.mapper.FFM.visualization import make_mapping\n",
    "from IPython.display import SVG\n",
    "newmapping = make_mapping(mappings.data.iloc[0], spec.workload.einsum_names, get_rank_variable_bounds_for_all_einsums(spec))\n",
    "b = {}\n",
    "for col in mappings.data.columns:\n",
    "    print(f'{col}: {mappings.data.iloc[0][col]}')\n",
    "    if \"Latency\" in col:\n",
    "        b[col] = mappings.data.iloc[0][col]\n",
    "display(SVG(newmapping.render()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([a, b])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
