# H100 FP8 SXM5
# 2000 TFLOPs
# 700W
# 814mm^2
# https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/#:~:text=The%20H100%20SXM5%20GPU%20has,analytics%2C%20but%20not%20graphics%20processing
# Peak TF32 Tensor TFLOPS 494.7
# Peak FP64 Tensor TFLOPS 66.9
# Peak INT8 Tensor TOPS 1978.9
# pJ/MAC = 700 / 1978.9 * 2 = 0.354
# https://resources.nvidia.com/en-us-tensor-core

arch:
  # ============================================================
  # Architecture Description
  # ============================================================
  version: "0.5"
  global_cycle_period: 1e-9
  nodes: # Top-level is hierarchical
  - !Memory
    name: MainMemory
    attributes:
      _size: inf
      _datawidth: 8
    actions:
    - {name: read, arguments: {energy: 7.03}}
    - {name: write, arguments: {energy: 7.03}}
    - {name: leak, arguments: {energy: 0}}

  - !Memory
    name: L2_cache
    attributes:
      _size: 50*1024*1024*8
      _datawidth: 8
    constraints:
      tensors:
        keep: ~MainMemory.tensors() | input | output
    actions:
    - {name: read, arguments: {energy: 1.88}}
    - {name: write, arguments: {energy: 2.36}}
    - {name: leak, arguments: {energy: 0}}

  - !Memory
    name: RF
    spatial: {fanout: {X: 132}}
    attributes:
      _size: (256*1024*8) # 256 kB
      _datawidth: 8
    constraints:
      tensors:
        keep: All()
    actions:
    - {name: read, arguments: {energy:  0.249}}
    - {name: write, arguments: {energy:  0.293}}
    - {name: leak, arguments: {energy: 0}}

  # - !Container
  #   name: tensor_core
  #   spatial: {meshX: 4}

  # - !Container # Calculated by GPU boost clock * # tensor cores / peak FP8 TOPs
  #              # Divide by 2 for FP16, divide by 4 for FP32
  #   name: macs_in_tensor_core
  #   spatial: {meshX: 512}

  - !Memory
    name: Register
    spatial: {fanout: {X: 128, Y: 256}}
    attributes:
      _size: 8 # 8b
      _datawidth: 8
    constraints:
      tensors:
        keep: weight
        bypass: ~weight
        # tile_shape:
        # - expression: weight.rank_variables()
        #   operator: ==
        #   value: 1
      dataflow:
        tensor_order_options:
        - [All()] # Outputs more stationary than inputs
      spatial:
      - name: X # Parallel rank
        loop_bounds:
        - expression: ~parallel_rank
          operator: ==
          value: 1
        # - expression: parallel_rank
        #   operator: ==
        #   value: 128
      - name: Y # Reduced rank. No iteration over outputs
        loop_bounds:
        - expression: ~reduced_rank
          operator: ==
          value: 1
        # - expression: reduced_rank
        #   operator: ==
    actions:
    - {name: read, arguments: {energy: 0}}
    - {name: write, arguments: {energy: 0}}
    - {name: leak, arguments: {energy: 0}}


  - !Compute
    name: mac
    actions:
    - {name: compute, arguments: {energy: 0}} #0.84}}
    - {name: leak, arguments: {energy: 0}}
