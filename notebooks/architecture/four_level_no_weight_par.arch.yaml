variables:
  global_cycle_seconds: 1e-9
  technology: "7nm"

architecture:
  version: "0.5"
  nodes:
  - !Memory
    name: MainMemory
    component_class: DRAM
    attributes:
      size: 9999999999999
      width: 4678
      shared_read_write_bandwidth: 614e9 * 8 / 1.05e9 # 614 GB/s. Chip runs at at 1.05GHz, so divide by 1.05 to get per-cycle bandwidth
      datawidth: 8
      area: 1
    constraints:
      tensors:
        keep: ~Intermediates()
    actions:
      # Upper end of the range from the TPU paper. The lower end came from their
      # reference, and they said it left out some things.
    - {name: read, arguments: {energy: 7.03e-12}}
    - {name: write, arguments: {energy: 7.03e-12}}
    - {name: leak, arguments: {energy: 0}}

  - !Memory
    name: GlobalBuffer
    component_class: SRAM
    attributes:
      size: 1024*1024*128*8 # 128MB
      width: 8192
      datawidth: 8
      area: 1
    constraints:
      tensors:
        keep: All() #~MainMemory.tensors() | input | output
        # bypass: weight - ( ~MainMemory.tensors() | input | output)
      # dataflow:
      #   tensor_order_options:
      #   # - [weight, ~weight]
      #   - [MainMemory.tensors(), ~MainMemory.tensors()]
      {% if FFMT | default(false) %}
      dataflow:
        tensor_order_options:
        - [weight, input, output]
        - [weight, output, input]
      {% endif %}
    actions:
    - {name: read, arguments: {energy: 1.88e-12}}
    - {name: write, arguments: {energy: 2.36e-12}}
    - {name: leak, arguments: {energy: 0}}

  # - !Memory
  #   name: LocalBuffer
  #   component_class: SRAM 
  #   spatial: {fanout: {X: 4}}
  #   attributes:
  #     size: 1024*1024*4*8 # 4MB
  #     width: 2048
  #     datawidth: 8
  #     area: 1
  #   constraints:
  #     tensors:
  #       keep: Nothing()
  #       bypass: All()
  #       # keep: input | output
  #       # bypass: weight
  #     dataflow:
  #       tensor_order_options:
  #       # - [All()] # No uneven. Put all tensors at the same level.
  #       - [input, output]
  #     spatial:
  #       - dimension: X
  #         min_utilization: 1
  #         loop_bounds:
  #         - expression: weight.rank_variables()
  #           operator: ==
  #           value: 1
  #         # - expression: b
  #         #   operator: ==
  #         #   value: 4
  #   actions:
  #   - {name: read, arguments: {energy: 0.249e-12}}
  #   - {name: write, arguments: {energy: 0.293e-12}}
  #   - {name: leak, arguments: {energy: 0}}

  # - !Memory
  #   name: Reg2
  #   component_class: SRAM
  #   attributes:
  #     size: 99999999999
  #     datawidth: 8
  #     area: 0
  #   constraints:
  #     tensors:
  #       keep: All()
  #       tile_shape:
  #       - expression: parallel_rank | reduced_rank
  #         operator: ==
  #         value: 128
  #       - expression: ~(parallel_rank | reduced_rank)
  #         operator: ==
  #         value: 1
  #     dataflow:
  #       tensor_order_options:
  #       - [input, output, weight]
  #   actions:
  #   - {name: read, arguments: {energy: 0}}
  #   - {name: write, arguments: {energy: 0}}
  #   - {name: leak, arguments: {energy: 0}}

  - !Memory
    name: Register
    component_class: dummy_storage
    spatial: {fanout: {X: 128, Y: 128, Z: 4}}
    attributes:
      size: 8 # 8b
      datawidth: 8
      width: 8
      depth: 1
      area: 1
    constraints:
      tensors:
        # keep: weight - GlobalBuffer.tensors()
        bypass: All() # ~weight | GlobalBuffer.tensors()
        # tile_shape:
        # - expression: weight.rank_variables()
        #   operator: ==
        #   value: 1
      dataflow:
        tensor_order_options:
        - [All()] # Outputs more stationary than inputs
      spatial:
      - dimension: X # Parallel rank
        min_utilization: (1 if (weight and input and output) else 0)
        loop_bounds:
        - expression: (input if (weight and input and output) else Nothing()).rank_variables() # | ~weight.rank_variables()
          operator: ==
          value: 1
      - dimension: Y # Reduced rank. No iteration over outputs
        min_utilization: (1 if (weight and input and output) else 0)
        loop_bounds:
        - expression: (output if (weight and input and output) else All()).rank_variables() # output.rank_variables() # | ~weight.rank_variables()
          operator: ==
          value: 1
      # - dimension: Z
        # min_utilization: (1 if (weight and input and output) else 0)
        # loop_bounds:
        # - expression: weight.rank_variables()
        #   operator: ==
        #   value: 1
        # - expression: b
        #   operator: ==
        #   value: 1
    actions:
    - {name: read, arguments: {energy: 0}}
    - {name: write, arguments: {energy: 0}}
    - {name: leak, arguments: {energy: 0}}

  - !Compute
    name: MAC
    component_class: intmac
    attributes: {multiplier_width: 8, adder_width: 16, area: 1}
    actions:
    - {name: compute, arguments: {energy: 0.84e-12 / 8}}
    - {name: leak, arguments: {energy: 0}}

component_classes:
  version: "0.5"         # REQUIRED version number
  component_classes:
  - name: intmac
    attributes:
      technology: REQUIRED
      multiplier_width: REQUIRED
      adder_width: REQUIRED
      global_cycle_seconds: REQUIRED
    subcomponents:
    - name: intadder
      component_class: aladdin_adder
      attributes: {width: adder_width}
    - name: intmultiplier
      component_class: aladdin_multiplier
      attributes: {width_a: multiplier_width, width_b: multiplier_width}
    actions:
    - name: compute
      subcomponents:
      - name: intadder
        actions: [name: read]
      - name: intmultiplier
        actions: [{name: read}]
    - name: leak
      subcomponents:
      - name: intadder
        actions: [{name: leak}]
      - name: intmultiplier
        actions: [{name: leak}]
