{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "from hwcomponents_cacti import SRAM as CactiSRAM\n",
    "from hwcomponents_library import AladdinAdder, AladdinMultiplier\n",
    "\n",
    "from fastfusion.frontend.architecture import Memory\n",
    "from fastfusion.frontend.specification import Specification\n",
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_sims\n",
    "from fastfusion.mapper.simanneal.wrappers import join_sims\n",
    "\n",
    "import copy\n",
    "import time\n",
    "from fastfusion import Specification\n",
    "from fastfusion.mapper.metrics import Metrics\n",
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_sims\n",
    "from fastfusion.mapper.FFM.joining.sim import SIM\n",
    "from fastfusion.mapper.FFM.joining.simexplore import join_sims\n",
    "import fastfusion.mapper.FFM.exploration.mapper_one_einsum as mapper_one_einsum\n",
    "\n",
    "from fastfusion.mapper.FFM.exploration.mapping_filter_tags.ffmt import get_ffmt_tag\n",
    "from fastfusion.mapper.FFM.exploration.mapping_filter_tags.onesplit import get_one_split_tag\n",
    "from fastfusion.mapper.FFM.pareto import PartialMappings\n",
    "from fastfusion.mapper.FFM import make_pmappings, join_pmappings\n",
    "from fastfusion.mapper.FFM.exploration.mapper_multi_einsum import get_per_tensor_size, get_num_computes\n",
    "\n",
    "\n",
    "objective = lambda df: df['Total_Latency']# * df['Total_Energy']\n",
    "\n",
    "def get_pmappings(spec: Specification, parameterization: str, local_buffer_fanout: int, batch_size=1, n_tokens=16384):\n",
    "    spec.mapper_ffm.metrics = Metrics.LATENCY# | Metrics.ENERGY\n",
    "    spec.calculate_component_energy_area(area=False)\n",
    "    llb: Memory = spec.architecture.nodes[\"LocalBuffer\"]\n",
    "    llb.spatial.fanout[\"Z\"] = local_buffer_fanout\n",
    "    cachekey = (parameterization, local_buffer_fanout, batch_size, n_tokens)\n",
    "    fname = \"_\".join(str(x) for x in cachekey) + \".pkl\"\n",
    "    \n",
    "    if parameterization == \"Ideal\":\n",
    "        llb: Memory = spec.architecture.nodes[\"LocalBuffer\"]\n",
    "        llb.spatial.fanout.clear()\n",
    "        reg: Memory = spec.architecture.nodes[\"Register\"]\n",
    "        reg.spatial.fanout.clear()\n",
    "        reg.attributes.size *= 128 * 128 * local_buffer_fanout\n",
    "        dram: Memory = spec.architecture.nodes[\"MainMemory\"]\n",
    "        dram.attributes.shared_read_write_bandwidth += f\" / (128 * 128 * {local_buffer_fanout})\"\n",
    "    \n",
    "    if os.path.exists(f\"cache/pmappings_{fname}.pkl\"):\n",
    "        print(f\"Loading from cache: pmappings_{fname}\")\n",
    "        return pickle.load(open(f\"cache/pmappings_{fname}.pkl\", \"rb\"))\n",
    "    pmappings = make_pmappings(spec)\n",
    "    pickle.dump(pmappings, open(f\"cache/pmappings_{fname}.pkl\", \"wb\"))\n",
    "    return pmappings\n",
    "\n",
    "def get_fused_mappings(local_buffer_fanout: int, parameterization=\"\", return_mappings=False, batch_size=1, n_tokens=16384):\n",
    "    spec = Specification.from_yaml(\n",
    "        f\"architecture/four_level.arch.yaml\",\n",
    "        \"workloads/mha_full.workload.yaml\",\n",
    "        \"workloads/mha_full.renames.yaml\",\n",
    "        jinja_parse_data={\n",
    "            \"BATCH_SIZE\": batch_size,\n",
    "            \"N_TOKENS\": n_tokens,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # if parameterization == \"Ideal\":\n",
    "    #     return get_num_computes(spec) / local_buffer_fanout / 128 / 128, None\n",
    "    \n",
    "    cachekey = (parameterization, local_buffer_fanout, batch_size, n_tokens)\n",
    "    fname = \"_\".join(str(x) for x in cachekey) + \".pkl\"\n",
    "    if os.path.exists(f\"cache/{fname}.pkl\"):\n",
    "        print(f\"Loading from cache: {fname}\")\n",
    "        mappings = pickle.load(open(f\"cache/{fname}.pkl\", \"rb\"))\n",
    "        if return_mappings:\n",
    "            return mappings\n",
    "        return objective(mappings.data).min(), mappings\n",
    "\n",
    "    spec = copy.deepcopy(spec)\n",
    "    if parameterization in [\"Unfused\", \"LoopTree\", \"LoopForest\"]:\n",
    "        pmappings = get_pmappings(spec, \"LoopForest\", local_buffer_fanout, batch_size, n_tokens)\n",
    "    elif parameterization == \"TileFlow\":\n",
    "        spec.mapper_ffm.timeloop_style_even = True\n",
    "        pmappings = get_pmappings(spec, parameterization, local_buffer_fanout, batch_size, n_tokens)\n",
    "    elif parameterization == \"Ideal\":\n",
    "        pmappings = get_pmappings(spec, parameterization, local_buffer_fanout, batch_size, n_tokens)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown parameterization: {parameterization}\")\n",
    "    \n",
    "    if parameterization == \"Unfused\":\n",
    "        filter_lambda = lambda pm: set(x.resource_name for x in pm.compatibility.tensors) == {\"MainMemory\"}\n",
    "        pmappings.filter(filter_lambda)\n",
    "    elif parameterization == \"LoopTree\" or parameterization == \"TileFlow\":\n",
    "        filter_lambda = lambda pm: len(set(len(x.loops) for x in pm.compatibility.tensors if x.resource_name != \"MainMemory\")) <= 1\n",
    "        pmappings.filter(filter_lambda)\n",
    "\n",
    "    mappings = join_pmappings(spec, pmappings)\n",
    "    \n",
    "    if parameterization == \"Ideal\":\n",
    "        mappings.data[\"Total_Latency\"] /= 128 * 128 * local_buffer_fanout\n",
    "    \n",
    "    pickle.dump(mappings, open(f\"cache/{fname}.pkl\", \"wb\"))\n",
    "    if return_mappings:\n",
    "        return mappings\n",
    "    \n",
    "    return objective(mappings.data).min(), mappings\n",
    "\n",
    "parameterization2edp = {}\n",
    "parameterization2mappings = {}\n",
    "\n",
    "parameterizations = [\"Unfused\", \"TileFlow\", \"LoopTree\", \"LoopForest\", \"Ideal\"]\n",
    "\n",
    "for batch_size in [1]:\n",
    "    # for batch_size, n_tokens in [(64, 2048), (1, 2048), (64, 16384), (1, 16384), (1, 1024), (1, 4096), (1, 8192), (1, 32768), (1, 65536)]:\n",
    "    for n_tokens in [1024, 2048, 4096, 8192, 16384, 32768]:#, 65536]:\n",
    "        for n_pes in [256]:\n",
    "            for parameterization in parameterizations:\n",
    "                x, mappings = get_fused_mappings(\n",
    "                    n_pes,\n",
    "                    parameterization=parameterization,\n",
    "                    batch_size=batch_size,\n",
    "                    n_tokens=n_tokens,\n",
    "                )\n",
    "                if x != 0:\n",
    "                    parameterization2edp.setdefault((batch_size, n_tokens, n_pes), {})[parameterization] = x\n",
    "                    parameterization2mappings.setdefault((batch_size, n_tokens, n_pes), {})[parameterization] = mappings\n",
    "                    print(f\"{batch_size} {n_tokens} {n_pes} {parameterization}: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "# plt.rcParams.update({'font.size': 28})\n",
    "\n",
    "def plot_default_formatting(ax, grid_axis='both'):\n",
    "    ax.tick_params(axis='both', which='major')#, labelsize=20)\n",
    "    ax.tick_params(axis='both', which='minor')#, labelsize=20)\n",
    "    legend = ax.legend()\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    legend.get_frame().set_edgecolor('black')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('black')\n",
    "    if ax.get_legend() is None:\n",
    "        legend = ax.legend(fontsize=24, ncol=2)\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(axis=grid_axis, which='major', linestyle='-', linewidth='0.3', color='gray')\n",
    "    ax.grid(axis=grid_axis, which='minor', linestyle='--', linewidth='0.1', color='lightgray')\n",
    "    \n",
    "\n",
    "def make_bar_chart(\n",
    "    data,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    "    y_scale,\n",
    "    output_file=None,\n",
    "    normalize: bool = False,\n",
    "    ylim=(None, None),\n",
    "    xlim=(None, None),\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a bar chart from the given data and save it as a PDF.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    if isinstance(data, dict) and isinstance(next(iter(data.values())), dict):\n",
    "        bar_width = 0.8 / len(data)\n",
    "        keys = list(next(iter(data.values())).keys())\n",
    "        x = range(len(keys))\n",
    "        first = next(iter(data.values()))\n",
    "            \n",
    "        for i, (label, values) in enumerate(data.items()):\n",
    "            bar_positions = [pos + i * bar_width for pos in x]\n",
    "            to_plot = values\n",
    "            if normalize:\n",
    "                to_plot = {k: v / first[k] for k, v in values.items()}\n",
    "            bars = plt.bar(bar_positions, to_plot.values(), width=bar_width, label=label)\n",
    "        plt.xticks([pos + (len(data) - 1) * bar_width / 2 for pos in x], keys)\n",
    "        plt.legend(loc='upper right', fontsize=10)\n",
    "    else:\n",
    "        keys = list(data.keys())\n",
    "        bars = plt.bar(keys, data.values())\n",
    "\n",
    "    # Set logarithmic scale for Y-axis if specified\n",
    "    if y_scale == 'log':\n",
    "        plt.yscale('log')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlim(xlim)\n",
    "\n",
    "    # Rotate X-axis labels vertically\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plot_default_formatting(plt.gca(), grid_axis='y')\n",
    "    \n",
    "    if output_file is not None:\n",
    "        with open(output_file, 'wb') as f:\n",
    "            plt.savefig(f, format='pdf', bbox_inches='tight')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "parameterization2edp_normalized = {}\n",
    "for (batch_size, n_tokens, n_pes), edp_dict in parameterization2edp.items():\n",
    "    min_edp = min(edp_dict.values())\n",
    "    ideal = edp_dict[\"Ideal\"]\n",
    "    min_non_ideal = min(v for k, v in edp_dict.items() if k != \"Ideal\")\n",
    "    for k, v in edp_dict.items():\n",
    "        parameterization2edp_normalized.setdefault(f\"{batch_size} {n_tokens} {n_pes} {k}\", v / min_edp)\n",
    "        print(f\"{batch_size} {n_tokens} {n_pes} {k}: {v / min_non_ideal:.4f} ({v / ideal:.4f})\")\n",
    "\n",
    "print()\n",
    "make_bar_chart(\n",
    "    parameterization2edp_normalized,\n",
    "    title=None,\n",
    "    xlabel=None,\n",
    "    ylabel=\"EDP\",\n",
    "    y_scale='linear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from fastfusion.mapper.FFM.pareto.df_convention import MAPPING_COLUMN\n",
    "mapping = parameterization2mappings[(1, 32768, 256)][\"LoopForest\"]\n",
    "display(SVG(mapping.data.iloc[0][MAPPING_COLUMN].render()))\n",
    "print(mapping.data.iloc[0][\"Total_Latency\"])\n",
    "for col in mapping.data.columns:\n",
    "    if \"latency\" not in col.lower():\n",
    "        continue\n",
    "    print(f'{col}: {mapping.data.iloc[0][col]} ({mapping.data.iloc[0][col] / mapping.data.iloc[0][\"Total_Latency\"] * 100:.2f}%)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
